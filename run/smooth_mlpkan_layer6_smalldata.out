nohup: ignoring input
begin to read data
Data Loading with 9.061 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─SiLU: 2-1                         --
│    └─KAN: 2-2                          --
│    │    └─GELU: 3-1                    --
│    │    └─ParameterList: 3-2           1,049,088
│    │    └─ParameterList: 3-3           8,392,704
│    │    └─ModuleList: 3-4              8,704
│    │    └─ModuleList: 3-5              2
├─FNO2d: 1-2                             --
│    └─GELU: 2-3                         --
│    └─Linear: 2-4                       64
│    └─ModuleList: 2-5                   --
│    │    └─SpectralConv2d: 3-6          663,552
│    │    └─SpectralConv2d: 3-7          663,552
│    │    └─SpectralConv2d: 3-8          663,552
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    └─ModuleList: 2-6                   --
│    │    └─Conv2d: 3-12                 1,056
│    │    └─Conv2d: 3-13                 1,056
│    │    └─Conv2d: 3-14                 1,056
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    └─Linear: 2-7                       4,224
│    └─Linear: 2-8                       516
=================================================================
Total params: 13,442,950
Trainable params: 13,442,950
Non-trainable params: 0
=================================================================
Training...
0 45.61008908599615 5.9276232879638675 0.08553301811218261
1 45.13726003281772 0.3975114292144775 0.055391273498535155
2 45.730987288057804 0.3200535316467285 0.046857805252075196
3 45.49045412801206 0.2856640968322754 0.045038700103759766
4 45.216803604736924 0.25722091941833497 0.04464580774307251
5 45.088054582476616 0.2432115354537964 0.03657583594322204
6 45.098657513037324 0.21210387134552003 0.0334717857837677
7 45.01260823942721 0.19522472076416017 0.03382383942604065
8 45.05892931111157 0.2184793676376343 0.03042514681816101
9 44.984442353248596 0.1643976369857788 0.025130059719085693
10 45.158771488815546 0.14549834232330322 0.024745683670043945
11 44.993587136268616 0.13282455339431762 0.022385988235473633
12 45.08673841878772 0.12414926853179932 0.021864945888519286
13 45.001640947535634 0.1305002908706665 0.023690928220748902
14 45.06734195910394 0.11688424234390259 0.019905536770820617
15 44.96907952800393 0.10430949325561524 0.022368640899658204
16 45.08552784845233 0.1000926881790161 0.018231201171875
17 45.026655128225684 0.09477187042236328 0.018787522315979004
18 45.083417205139995 0.09503807106018067 0.016229400634765623
19 45.006407499313354 0.08755926070213318 0.017218239307403564
20 45.16173696704209 0.08131716904640197 0.01581553280353546
21 45.02121876180172 0.08191777601242066 0.01579498291015625
22 45.06129518896341 0.07857479228973388 0.014621858596801757
23 45.02346317283809 0.07504151892662049 0.014627867341041566
24 45.042944356799126 0.07287295832633972 0.015436481237411499
25 45.040076207369566 0.0760510558128357 0.014476956725120544
26 45.03397451899946 0.06820183234214783 0.013455649614334106
27 45.012140333652496 0.06979973392486573 0.01304194688796997
28 44.997838757932186 0.06526442089080811 0.013500563502311706
29 45.0172840282321 0.06219767413139343 0.012737552523612977
30 45.06425360031426 0.06000678424835205 0.012878701090812683
31 45.0035913605243 0.060834129667282105 0.012302246689796448
32 44.95626944862306 0.05934763569831848 0.012791974544525147
33 45.03118055127561 0.05884126343727112 0.012146618366241455
34 44.92378082871437 0.056614743757247926 0.012068721055984498
35 45.00456438399851 0.05598612389564514 0.011721323728561401
36 44.91719244606793 0.05539325590133667 0.011253973841667176
37 45.00226358696818 0.05444158124923706 0.011364701986312866
38 44.918985256925225 0.05159808106422424 0.011165078282356262
39 44.95723302103579 0.05025105514526367 0.011347078680992127
40 44.9990560580045 0.051100594425201415 0.010726263523101806
41 44.948628226295114 0.05215358276367187 0.011149794459342957
42 44.89385034888983 0.052349641561508176 0.010993191599845886
43 44.95337085984647 0.04952749433517456 0.010672289133071899
44 44.87301754206419 0.04858579955101013 0.010795138478279113
45 44.92984781600535 0.048725458908081054 0.010383273363113404
46 44.89655327796936 0.04846830987930298 0.010511874556541442
47 44.95349793508649 0.05231029229164123 0.010037294626235961
48 44.890835870057344 0.043146003127098084 0.010578097105026244
49 44.91265266947448 0.04432782244682312 0.009893787801265716
50 45.02731507644057 0.047681167125701904 0.00985019475221634
51 44.91022059135139 0.04360276198387146 0.010138055086135864
52 44.92503243498504 0.04446636250019073 0.010148342847824097
53 44.9036112036556 0.04223166012763977 0.009686783850193025
54 44.93792398646474 0.042330078649520875 0.009647496342658997
55 44.87951537966728 0.0429891517162323 0.009849333763122558
56 44.97787240892649 0.04186277756690979 0.00937440812587738
57 44.85164196416736 0.04216976108551025 0.009622906446456908
58 44.93522464111447 0.04104162347316742 0.009367824792861938
59 44.88517556339502 0.04272379705905914 0.009344508051872253
60 45.049593564122915 0.041319146656990054 0.009276107251644135
61 44.96780348196626 0.039228629541397095 0.00995609164237976
62 45.06627632305026 0.04128894979953766 0.009154628217220306
63 44.98871146142483 0.03936045157909393 0.008743557333946227
64 44.97110687941313 0.03986206374168396 0.009735769927501678
65 44.91319037787616 0.03913355479240418 0.008958581984043121
66 45.00406323932111 0.03675421211719513 0.008843304812908173
67 45.06119909696281 0.03670067083835602 0.008684237599372863
68 45.01577004417777 0.036872148609161375 0.009084928035736083
69 44.95099914073944 0.04021852955818176 0.009142161011695862
70 45.10611277259886 0.03724887399673462 0.009211135506629943
71 44.99414299800992 0.03491758036613464 0.008965194523334503
72 45.01596689410508 0.03801242680549621 0.00905020147562027
73 44.9782456997782 0.03528109843730926 0.009099495708942413
74 45.164514776319265 0.03643929107189178 0.008449325859546662
75 45.00811092928052 0.03376538913249969 0.008711266219615937
76 45.041557382792234 0.035575496649742126 0.009220121502876282
77 45.08239987678826 0.03745073037147522 0.008749738335609436
78 45.05644025653601 0.03569750440120697 0.009283789694309234
79 45.11954718455672 0.03488782434463501 0.009299210906028747
80 45.25842018797994 0.035470854568481446 0.008408816456794739
81 45.26116241142154 0.033338996863365174 0.008274186253547668
82 45.074661899358034 0.033411463356018065 0.009194466471672057
83 45.284203447401524 0.03379218337535858 0.008255449533462524
84 45.07934953272343 0.031948518657684326 0.00870463103055954
85 45.242242347449064 0.03153584103584289 0.008055428564548493
86 45.0672002248466 0.0319658326625824 0.008946654796600341
87 45.15441191568971 0.031750610518455506 0.008355695009231567
88 45.08087352477014 0.03277803375720978 0.008462982475757599
89 45.163573663681746 0.03305803301334381 0.008296396434307098
90 45.163199592381716 0.03326444094181061 0.008282739818096161
91 45.15516516752541 0.033001704263687134 0.008286474645137787
92 45.174435172230005 0.031849609923362734 0.008036985099315642
93 45.13897011242807 0.029964252257347106 0.008102299571037292
94 45.073107888922095 0.02973901093006134 0.008373266160488129
95 45.27265062741935 0.02915482952594757 0.007923144400119781
96 45.06100252829492 0.029796350049972536 0.00814625084400177
97 45.35662718489766 0.03067712914943695 0.00782313734292984
98 45.065288277342916 0.02951851260662079 0.008506950438022614
99 45.12510399706662 0.030374096179008485 0.00792418360710144
100 45.15665485151112 0.02942942819595337 0.008061572015285491/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

101 45.111000495031476 0.029161777424812317 0.007828168272972107
102 45.05335738323629 0.030998107242584227 0.007966819703578949
103 45.230672512203455 0.02822978355884552 0.007819384634494782
104 45.03811158798635 0.0279960905790329 0.007846782505512238
105 45.06133730709553 0.029931654930114748 0.008352391719818116
106 45.06023324094713 0.030160697770118712 0.008600345849990844
107 45.23182667978108 0.03036402003765106 0.007702212333679199
108 45.03880424425006 0.027303731846809388 0.00790663093328476
109 45.03390047326684 0.02902087323665619 0.00773756206035614
110 45.25613802857697 0.027592403531074523 0.007602818608283997
111 45.01038500480354 0.029232930541038513 0.008276399672031403
112 45.142516979947686 0.02731214246749878 0.007521379292011261
113 44.98228926770389 0.02829186487197876 0.00819111704826355
114 45.04454178735614 0.02778279528617859 0.007534546256065369
115 44.93636740744114 0.026787016010284423 0.007529029250144958
116 45.02458685077727 0.025695351338386536 0.007730004489421845
117 44.9514520727098 0.027794751644134523 0.007938782572746278
118 45.002351662144065 0.026397117948532105 0.00761977881193161
119 44.92870418727398 0.026438777565956115 0.007796618938446045
120 45.06986502557993 0.02784344391822815 0.007669806778430939
121 45.009609907865524 0.0268730073928833 0.007292905151844025
122 44.967004330828786 0.02528481342792511 0.007485372722148895
123 44.88913356512785 0.025318485474586486 0.007777326107025147
124 44.94543873332441 0.026106698751449584 0.00755884051322937
125 44.882019290700555 0.025854160189628602 0.00736680269241333
126 44.94631941244006 0.026038186144828796 0.007563736438751221
127 44.874215172603726 0.0256815856218338 0.007420921921730042
128 44.93747524544597 0.025257046389579773 0.007741490304470062
129 44.88857185468078 0.025081641817092895 0.007829808592796327
130 45.2191098164767 0.025450660181045533 0.007200070917606354
131 44.875174049288034 0.026892063903808595 0.007891646921634674
132 44.93026330135763 0.025246308517456056 0.007603399157524109
133 44.91525990143418 0.028421339440345764 0.0073956727981567385
134 44.902224607765675 0.023782770967483522 0.007295083105564117
135 44.92722163721919 0.023668662643432616 0.00734151840209961
136 44.91952614672482 0.024894694972038268 0.007616731822490692
137 44.940971951931715 0.024665099620819093 0.007453604638576508
138 45.02823338843882 0.023510714101791383 0.007049305140972137
139 44.95571434125304 0.02339291684627533 0.0071515366435050964
140 44.967850686982274 0.023738396644592284 0.007185907065868378
141 44.96425176598132 0.02325342719554901 0.007719794511795044
142 44.87815897166729 0.024123833322525025 0.0073069331049919126
143 44.966368336230516 0.023480723214149474 0.007268899977207184
144 44.86101230047643 0.02445864381790161 0.007449710071086884
145 44.97908801026642 0.023439187681674956 0.007184889018535614
146 44.887789115309715 0.02270485817193985 0.007210791110992432
147 44.97662987187505 0.024456678223609925 0.007337689399719238
148 44.885532731190324 0.02336335517168045 0.0072913807630538945
Early stop at epoch 149
Training Time:6765.606s
