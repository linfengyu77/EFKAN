nohup: ignoring input
begin to read data
Data Loading with 9.335 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─SiLU: 2-1                         --
│    └─KAN: 2-2                          --
│    │    └─GELU: 3-1                    --
│    │    └─ParameterList: 3-2           524,544
│    │    └─ParameterList: 3-3           4,196,352
│    │    └─ModuleList: 3-4              8,448
│    │    └─ModuleList: 3-5              2
├─FNO2d: 1-2                             --
│    └─GELU: 2-3                         --
│    └─Linear: 2-4                       64
│    └─ModuleList: 2-5                   --
│    │    └─SpectralConv2d: 3-6          663,552
│    │    └─SpectralConv2d: 3-7          663,552
│    │    └─SpectralConv2d: 3-8          663,552
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    └─ModuleList: 2-6                   --
│    │    └─Conv2d: 3-12                 1,056
│    │    └─Conv2d: 3-13                 1,056
│    │    └─Conv2d: 3-14                 1,056
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    └─Linear: 2-7                       4,224
│    └─Linear: 2-8                       516
=================================================================
Total params: 8,721,798
Trainable params: 8,721,798
Non-trainable params: 0
=================================================================
Training...
0 125.70017837174237 1.8484488438288371 0.050581387360890705
1 128.1373451668769 0.2704865573247274 0.03680902548631032
2 128.89195583760738 0.17972112515767416 0.025567025542259217
3 129.124692004174 0.14201103153228758 0.022151087562243144
4 129.22087644785643 0.1170738556543986 0.018913305660088857
5 129.28095804154873 0.10352324250539144 0.01671261316537857
6 129.15403400920331 0.0912122764269511 0.015158405244350434
7 129.27676377817988 0.08251229662895203 0.014505776723225911
8 129.16915737092495 0.07453825079600017 0.013345884581406911
9 129.18022917211056 0.07134184172948202 0.012882968227068583
10 128.90224073268473 0.06626594479878743 0.011793562571207682
11 128.6105496957898 0.06337060116132101 0.011305985311667124
12 128.68910778313875 0.06093203959465027 0.010839157442251841
13 128.8002641480416 0.058974324226379396 0.011365076124668121
14 128.63585908524692 0.05651466995875041 0.010652196655670802
15 128.72071051597595 0.05283414400418599 0.010070919771989187
16 128.74139772355556 0.05220573401451111 0.009706721315781276
17 128.77725757285953 0.05018358956972758 0.009015155990918478
18 128.9304073061794 0.047598703797658286 0.008882032175858815
19 129.10532625950873 0.04813231026331584 0.009084517776966094
20 129.34677389077842 0.04577945098876953 0.008299122035503387
21 129.17297073453665 0.04408820312023163 0.00822598041097323
22 129.16053443215787 0.04468821993668874 0.008391794443130493
23 129.27081515081227 0.04202891100247701 0.008020201633373896
24 129.1430685725063 0.04020639053980509 0.008020027230183283
25 129.25639379397035 0.04203670293490092 0.008363036225239436
26 129.1728046797216 0.0401174435377121 0.007564796298742294
27 129.07151555642486 0.03960844451586405 0.007567355165878932
28 129.27939360961318 0.038552587294578555 0.007542848318815232
29 129.19958709552884 0.03744417578379313 0.007589891860882441
30 128.93064685724676 0.03715339794953664 0.007191244711478551
31 128.66768565028906 0.03528322676022848 0.007130719155073166
32 128.67348349653184 0.035618559002876284 0.00705741548538208
33 128.71432072855532 0.03414145260651906 0.007521794180075328
34 128.62116371840239 0.03352631124655406 0.007244480719168981
35 128.6750524621457 0.03365007402102153 0.006744720677534739
36 128.81193555891514 0.0329716695467631 0.006318306565284729
37 128.67204955220222 0.03294042919476827 0.006493015031019847
38 128.8506032191217 0.031991804893811544 0.006700565119584402
39 129.03594414144754 0.03162945956389109 0.006566401749849319
40 129.2734383046627 0.03191603310108185 0.0062002516984939575
41 129.32062703184783 0.03224676824410756 0.005984229236841202
42 129.2706720288843 0.029033689951896667 0.006125097960233688
43 129.27681329473853 0.02926857421398163 0.005853977034489314
44 129.22075784020126 0.029343826985359193 0.0063527605930964156
45 129.16145896539092 0.029149328724543254 0.00622486412525177
46 129.25569328293204 0.028646801360448202 0.005832125782966614
47 129.15436143800616 0.027902355925242105 0.005949950416882833
48 129.24870637618005 0.028384111889203388 0.00602112893263499
49 129.1778857000172 0.027408510438601177 0.005729793429374695
50 128.8166704121977 0.027280663871765136 0.005977215677499771
51 128.75361010432243 0.028050596594810486 0.006219552566607793
52 128.63349892571568 0.026646205965677898 0.005656513671080271
53 128.66866445355117 0.026124692527453103 0.00550884818037351
54 128.68658101558685 0.02599065231482188 0.0058337473273277285
55 128.65567727945745 0.026061762436230977 0.005876544257005056
56 128.72921762242913 0.025516016753514607 0.00532338152329127
57 128.6001032050699 0.02482374145189921 0.0054389334370692575
58 128.7465721834451 0.025146434950828554 0.005358023444811503
59 128.885523468256 0.025993078859647117 0.005350552846988042
60 129.2008657976985 0.02412552539507548 0.005248056004444758
61 129.28803931176662 0.025188828094800313 0.0053833014070987706
62 129.33329522237182 0.024161855586369834 0.005266148785750071
63 129.17467661201954 0.02447543840408325 0.005481676866610845
64 129.33447599969804 0.023340907402833302 0.0050648041119178136
65 129.34083372354507 0.023799382042884825 0.004915854026873907
66 129.2191262282431 0.02306174587806066 0.005083560278018316
67 129.26303424686193 0.023476106178760527 0.005481981337070465
68 129.07015976123512 0.022723459593455 0.005259205331405004
69 129.2589651234448 0.022391480406125385 0.005165324201186498
70 129.2805504705757 0.02221236923535665 0.005070193474491437
71 128.9437479581684 0.022375664218266804 0.004818994209170342
72 128.7287680543959 0.02208670889933904 0.005266390571991603
73 128.64118756726384 0.022422686660289765 0.004970819006363551
74 128.71752982772887 0.02173726358016332 0.004972677449385325
75 128.72349220514297 0.021253554423650105 0.005108855023980141
76 128.64607221260667 0.02170358063777288 0.00488892820974191
77 128.7364217415452 0.02139582908153534 0.004934034133950869
78 128.63462440297008 0.021774869656562806 0.0048712377846241
79 128.65711084008217 0.02148292706012726 0.00502064382036527
80 128.8478326536715 0.020489324740568797 0.004782834693789482
81 128.76730601675808 0.020779887688159943 0.004945262675484022
82 129.28203678503633 0.020420733777681988 0.004709274088342984
83 129.31301361881196 0.02176783950328827 0.005197557896375656
84 129.25219919905066 0.020282310163974762 0.004911290546258291
85 129.4800457265228 0.020060556463400523 0.004635200018684069
86 129.28357341326773 0.020505393719673158 0.005335249851147333
87 129.37250235117972 0.02073552883863449 0.004712260300914446
88 129.3021027557552 0.019599480072657268 0.004919461364547411
89 129.30531851761043 0.019996724998950957 0.00461134106417497
90 129.42077284120023 0.02000897295475006 0.004587714403867722
91 129.29240495339036 0.019622888815402986 0.00464094004034996
92 129.35913322493434 0.019661764041582742 0.00515382286409537
93 129.329569991678 0.018954215673605602 0.004441872249046961
94 128.70504176989198 0.018790509804089865 0.0044434982389211656
95 128.73090717568994 0.019322734264532727 0.004549129183093707
96 128.64840235933661 0.019649467941125234 0.005072182223200798
97 128.64907077886164 0.019404188974698384 0.004862709552049637
98 128.71965574659407 0.019257275768121085 0.0047082229604323705
99 128.65886814519763 0.01876993205944697/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
 0.004597996488213539
100 128.9357817415148 0.01864266088803609 0.004470164274175962
101 128.85422117821872 0.018607244062423707 0.004593144625425339
102 129.0206031333655 0.01876498788992564 0.0043384142021338145
103 129.32219888828695 0.018043235810597736 0.0042983519732952115
104 129.3123725503683 0.018159760129451752 0.004269694308439891
105 129.3791166432202 0.01868076178630193 0.00443077419201533
106 129.2599123250693 0.018515990058581035 0.004395088692506154
107 129.29087255895138 0.01950903832912445 0.004418059378862381
108 129.43909630924463 0.017801021885871886 0.004230088571707408
109 129.5271039903164 0.018101502025127413 0.00433368160823981
110 129.3950052075088 0.0179565043648084 0.0043581488827864326
111 129.30650101415813 0.017489185106754304 0.004208676884571712
112 129.1772142071277 0.01769795527458191 0.00459767551223437
113 129.3672696016729 0.01751161631345749 0.004344725082317988
114 129.04716557450593 0.017584152686595918 0.004611203317840894
115 128.83305495791137 0.01790176760752996 0.004257260377208392
116 128.68994325771928 0.017775691366195678 0.004162117898464203
117 128.60288591496646 0.017601915391286214 0.00421672331293424
118 128.8015438914299 0.01741356307665507 0.0041387525697549185
119 128.73613159544766 0.017421259411176046 0.0041084623585144675
120 128.81122639216483 0.017057111032803853 0.004222229475776354
121 128.86412355676293 0.016935565292835235 0.0041519494156042735
122 128.72789092734456 0.016787324289480845 0.004224186400572459
123 128.87835619226098 0.016709361883004505 0.004219952051838239
124 129.12330569140613 0.0172012721657753 0.004268441652258237
125 129.34083680808544 0.01746457666953405 0.00405804830789566
126 129.3562866076827 0.016425785322984058 0.004264260098338127
127 129.2274698894471 0.016588579857349395 0.0041773980061213176
128 129.2221409380436 0.01664164330561956 0.004610383922855059
129 129.34934766590595 0.016429514340559642 0.004134499505162239
130 129.3003467041999 0.01719632824261983 0.004111878405014674
131 129.3037031274289 0.01635065842469533 0.004313539827863375
132 129.2414860855788 0.016156930883725485 0.0039714330087105435
133 129.15938668325543 0.016128649858633676 0.003978143711884817
134 129.36390054412186 0.015962376113732656 0.004125331923365593
135 128.70920002274215 0.01579692690769831 0.004543285702665647
136 128.7258251644671 0.016455818017323813 0.004082233329614004
137 128.66866201348603 0.016261298489570617 0.004030702610810598
138 128.62739549577236 0.015804051156838736 0.004371629496415456
139 128.7275060378015 0.01578594741423925 0.003993421072761218
140 128.7126618400216 0.016173481408754985 0.0040910519609848655
141 128.67640323005617 0.01582342665195465 0.004032029221455256
142 128.7296859715134 0.016176432542006174 0.00417872978746891
Early stop at epoch 143
Training Time:18584.884s
