nohup: ignoring input
/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
begin to read data
Data Loading with 8.412 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─Tanh: 2-1                         --
│    └─ModuleList: 2-2                   --
│    │    └─Linear: 3-1                  384
│    │    └─Linear: 3-2                  16,512
│    │    └─Linear: 3-3                  16,512
│    │    └─Linear: 3-4                  528,384
│    └─ModuleList: 2-3                   --
│    │    └─BatchNorm1d: 3-5             256
│    │    └─BatchNorm1d: 3-6             256
│    │    └─BatchNorm1d: 3-7             256
│    │    └─BatchNorm1d: 3-8             8,192
├─FNO2d: 1-2                             --
│    └─GELU: 2-4                         --
│    └─Linear: 2-5                       64
│    └─ModuleList: 2-6                   --
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    │    └─SpectralConv2d: 3-12         663,552
│    │    └─SpectralConv2d: 3-13         663,552
│    │    └─SpectralConv2d: 3-14         663,552
│    └─ModuleList: 2-7                   --
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    │    └─Conv2d: 3-18                 1,056
│    │    └─Conv2d: 3-19                 1,056
│    │    └─Conv2d: 3-20                 1,056
│    └─Linear: 2-8                       4,224
│    └─Linear: 2-9                       516
=================================================================
Total params: 4,563,204
Trainable params: 4,563,204
Non-trainable params: 0
=================================================================
Training...
0 85.3729507997632 0.5889650701522827 0.053243007659912106
1 84.88219262473285 0.2385402411142985 0.038128401041030886
2 84.0631341803819 0.186206685034434 0.031719974279403686
3 83.36085356585681 0.15719880800247193 0.02738077163696289
4 83.34018198214471 0.13562166763941447 0.023308864831924438
5 83.3311086203903 0.12236790444056193 0.02092848777770996
6 83.22121919132769 0.11336409902572632 0.019001811742782593
7 83.29970457218587 0.10365812428792318 0.018036925196647645
8 83.24825452268124 0.0988551144917806 0.01797487437725067
9 83.3103053662926 0.09113386888504028 0.016389651298522948
10 83.3060608599335 0.08714087150891622 0.015538111925125123
11 83.16848490200937 0.08278288815816244 0.016114817261695863
12 83.22925120778382 0.08006573977470398 0.014413948655128479
13 83.1452142894268 0.07704088546435038 0.014247121810913086
14 83.25026171468198 0.07325704838434856 0.013203938603401183
15 83.20186699740589 0.13457420681317647 0.01313621997833252
16 83.3001349195838 0.06785285228093466 0.01247406840324402
17 83.26857304759324 0.06499798625310262 0.011651374101638794
18 83.22208427265286 0.06347330592473348 0.012143983840942382
19 83.29554483480752 0.06364059405326843 0.012018522620201111
20 83.24555854871869 0.06139282493591309 0.01219361662864685
21 83.19872028939426 0.06078662017186483 0.011339055299758911
22 83.30582525581121 0.059283094247182214 0.011025953888893127
23 83.18248647451401 0.059056365696589154 0.011000383496284485
24 83.29136529751122 0.05683255969683329 0.010527200698852539
25 83.30683519504964 0.05542450033823649 0.010952772498130798
26 83.19246695935726 0.05370770977338155 0.010820380449295043
27 83.25218517147005 0.05410356098810832 0.011395263075828553
28 83.15965116582811 0.16073326258659362 0.012278190851211547
29 83.31210273690522 0.06122339301109314 0.011410018801689148
30 83.30995348095894 0.055911285654703775 0.010634673237800598
31 83.24230272322893 0.05357976924578349 0.009922415912151338
32 83.34135762788355 0.0531411718527476 0.010559996366500854
33 83.29047768935561 0.051449833250045776 0.009833504557609557
34 83.20312099345028 0.050110676940282185 0.009973353147506714
35 83.3184456359595 0.05032132611274719 0.010487733483314514
36 83.28853911161423 0.050624276113510135 0.00941693753004074
37 83.19608208350837 0.04846912194887797 0.009755077958106994
38 83.29022702015936 0.13759576384226482 0.017579893469810485
39 83.27936415374279 0.062373837963740034 0.009902126193046569
40 83.2383704148233 0.04910783422787984 0.009448754489421845
41 83.24306779541075 0.0475689258257548 0.009152815341949463
42 83.1283558756113 0.046760719696680705 0.009129235744476319
43 83.17374251037836 0.0467923144976298 0.0092723149061203
44 83.0525072477758 0.046071189880371094 0.008959934115409851
45 83.19657018966973 0.04473441196282705 0.009579493403434754
46 83.18604297935963 0.04546093124548594 0.009173183739185332
47 83.07650852948427 0.04393105250994364 0.008879792094230652
48 83.1296715773642 0.0441224174340566 0.008769086599349975
49 83.12919720076025 0.043886606280008955 0.009009100496768951
50 83.21321927011013 0.043404697426160176 0.008795939087867737
51 83.12597226910293 0.04202334292729696 0.008730704486370087
52 83.19824661687016 0.04168727791309357 0.009058770835399627
53 83.19606565125287 0.14656868788401287 0.013438050150871277
54 83.20170504413545 0.05306992201805115 0.008828375339508057
55 83.25961400382221 0.04236324557463328 0.008346877098083495
56 83.22884203866124 0.040514530126253764 0.008362357914447784
57 83.32023120112717 0.041302429842948914 0.008305213749408721
58 83.18488130532205 0.039774877325693764 0.007971384823322296
59 83.25449365377426 0.039520496169726055 0.008330972492694854
60 83.27497312054038 0.040951925508181254 0.00808190494775772
61 83.40774083510041 0.039509112294514974 0.00821366548538208
62 83.35424622893333 0.03849189302921295 0.007800430953502655
63 83.34929365105927 0.039188486011823016 0.008058936893939972
64 83.31476564332843 0.038644297854105635 0.007810372710227966
65 83.36567064002156 0.04007134379545848 0.008240131437778473
66 83.37871424853802 0.037837738966941835 0.008154379427433014
67 83.32481208629906 0.038200831802686054 0.0078025802969932554
68 83.46463926509023 0.03713290068308513 0.007792308032512665
69 83.36382042057812 0.037126079074541726 0.007929995357990265
70 83.48343190550804 0.03666647166411082 0.007709271013736725
71 83.37166798114777 0.06993782858053843 0.007074928283691407
72 83.22204967774451 0.030778618288040162 0.007128856778144836
73 83.34335148334503 0.03211797595024109 0.00718044251203537
74 83.19544028863311 0.03203368747234345 0.007283057570457458
75 83.24688750691712 0.03221125458876292 0.007123747169971466
76 83.23139247484505 0.03293488802115122 0.007496344745159149
77 83.14003561623394 0.03349045235315959 0.007402892708778381
78 83.18982467800379 0.03331414053440094 0.0073843058943748475
79 83.11894824914634 0.03438936809698741 0.007860668301582337
80 83.22032263688743 0.03492761046886444 0.007423286437988281
81 83.08232534490526 0.0332935622771581 0.007519465684890747
Early stop at epoch 82
Training Time:6924.173s
