nohup: ignoring input
begin to read data
Data Loading with 8.764 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─SiLU: 2-1                         --
│    └─KAN: 2-2                          --
│    │    └─GELU: 3-1                    --
│    │    └─ParameterList: 3-2           1,049,088
│    │    └─ParameterList: 3-3           8,392,704
│    │    └─ModuleList: 3-4              8,704
│    │    └─ModuleList: 3-5              2
├─FNO2d: 1-2                             --
│    └─GELU: 2-3                         --
│    └─Linear: 2-4                       64
│    └─ModuleList: 2-5                   --
│    │    └─SpectralConv2d: 3-6          663,552
│    └─ModuleList: 2-6                   --
│    │    └─Conv2d: 3-7                  1,056
│    └─Linear: 2-7                       4,224
│    └─Linear: 2-8                       516
=================================================================
Total params: 10,119,910
Trainable params: 10,119,910
Non-trainable params: 0
=================================================================
Training...
0 85.04293895326555 2.1079181814829506 0.07428066500027974
1 84.59337262809277 0.3863011447906494 0.05832065153121948
2 84.48704226501286 0.2829409859975179 0.045652283509572344
3 84.33691080659628 0.22497013556162515 0.03582927350203196
4 84.31983328983188 0.19077608801523843 0.031154212832450866
5 84.37348782457411 0.16872486667633058 0.02879239054520925
6 84.25293505936861 0.156056548055013 0.02718796650568644
7 84.32263225317001 0.14579472360610962 0.02588376446564992
8 84.40663068927824 0.14068525228500367 0.025239872733751934
9 84.52770089171827 0.13288173332214356 0.023236833572387695
10 84.52514412254095 0.12951783383687338 0.02271645478407542
11 84.5064154714346 0.12483690900802612 0.022657669067382813
12 84.70419153943658 0.12117137533823649 0.021175176004568735
13 84.95754728838801 0.117676611328125 0.021155411044756572
14 85.43354588747025 0.11404859380722046 0.02041684204339981
15 85.8705749437213 0.11202764495213827 0.019570375303427378
16 85.14261898770928 0.10787043476104737 0.020290975391864776
17 84.32904855161905 0.1074726051012675 0.01893189795811971
18 83.97312274761498 0.10222523539861043 0.018781322399775188
19 83.84789036400616 0.10195214751561482 0.01841968423128128
20 83.88988989591599 0.09945538892745971 0.017760207811991374
21 83.87736093997955 0.09692872063318889 0.017905703981717428
22 83.79982016049325 0.09535250727335612 0.017621119936307272
23 83.70066893845797 0.0946840833346049 0.01680318276087443
24 83.6994454190135 0.09232003482182821 0.016487077673276264
25 83.67678475938737 0.09090588722229004 0.016320640881856283
26 83.7345790695399 0.08903043697675068 0.01626888632774353
27 83.6824074164033 0.08745308585166932 0.01579529106616974
28 83.67450911179185 0.08661999909083049 0.015369629263877868
29 83.65236107632518 0.08481079004605611 0.0156543883283933
30 83.7819045279175 0.08329329992930094 0.015354580203692118
31 83.69542214274406 0.08280739057858785 0.014771471261978149
32 83.67341377213597 0.08109972178141275 0.01510857896010081
33 83.6459826938808 0.08052994988759359 0.014885124584039053
34 83.66713494062424 0.07846089596748353 0.01428700981537501
35 83.72123908251524 0.07776147147814433 0.014070408821105957
36 83.65159102715552 0.07747989684740703 0.014237638533115387
37 83.6796387694776 0.07560822679201762 0.01337569409608841
38 83.71103272400796 0.07458418911298116 0.013588383058706919
39 83.79435325786471 0.0735318656762441 0.013844597121079763
40 83.90652024373412 0.07295040826797486 0.013119987150033315
41 83.87416306138039 0.0722935762087504 0.01333132932583491
42 83.98226801492274 0.07103757937749226 0.013381592492262522
43 84.10144654661417 0.07010769278208415 0.012938149412473043
44 84.0925917737186 0.06948295408884685 0.013052877048651377
45 84.0742409247905 0.06926906639734903 0.01242901490132014
46 84.10604864731431 0.06830199128786722 0.013063691000143688
47 84.19815437868237 0.06682290681203207 0.012461302697658539
48 84.31100456789136 0.06675433144569397 0.012236584365367889
49 84.24669237621129 0.06620129707654317 0.011885281622409821
50 84.29063136316836 0.06625151745478312 0.011996860047181448
51 84.2548664379865 0.06521091394424439 0.01175247226158778
52 84.33958852291107 0.0642304386138916 0.011840696056683859
53 84.27842732146382 0.06380418424606324 0.011761226018269857
54 84.3085574042052 0.06321818871498108 0.011576365252335866
55 84.28539730422199 0.06281674216588339 0.01181930700937907
56 84.41407927684486 0.06230412138303121 0.011606151262919109
57 84.30602335743606 0.06160761688550313 0.011513947129249573
58 84.2312105614692 0.06204816546440124 0.011338634848594665
59 84.15366915613413 0.06113183139165242 0.011073071261246998
60 84.2021427359432 0.06021826683680216 0.0108866173128287
61 84.0161205381155 0.06008588228225708 0.011107153912385304
62 83.92558316886425 0.05938392427762349 0.011097942094008127
63 83.93021741509438 0.0602796483039856 0.010977135916550954
64 83.97085071541369 0.05864856859842936 0.010874619394540787
65 83.99242699891329 0.05858009490966797 0.010648526867230734
66 83.7943213544786 0.05783948884010315 0.010917042672634126
67 83.79611709155142 0.0574057030359904 0.011069131433963776
68 83.83796599693596 0.05899179824193319 0.0108642504910628
69 83.74969933927059 0.057187340132395424 0.010664045761028926
70 83.84318784810603 0.05698673367500305 0.010549817045529684
71 83.75258759967983 0.05635325285593669 0.010518507500489552
72 83.76879494823515 0.0557989023844401 0.010276733646790187
73 83.7202597502619 0.05604074713389079 0.010535006672143936
74 83.76706337369978 0.05554597601890564 0.010086420009533564
75 83.66631625592709 0.05505492720603943 0.010715401142835618
76 83.74724941700697 0.05506350571314494 0.010192278742790222
77 83.68202090635896 0.05464096455574036 0.010183675408363343
78 83.87730485573411 0.05482288459142049 0.01007451472679774
79 83.71428839303553 0.05384377455711365 0.010001904884974162
80 83.82692592032254 0.0538922709941864 0.00989235884944598
81 83.75892832875252 0.05374125768343608 0.01019529464840889
82 83.70880388841033 0.05311488450368245 0.009990099529425303
83 83.7749132514 0.05264984695116679 0.009951896627744039
84 84.10627671331167 0.053063200505574545 0.009975868910551071
85 84.61926945671439 0.05265144305229187 0.010288019547859828
86 84.84903438389301 0.0520860462029775 0.009638658831516902
87 84.89876390621066 0.05230580770174662 0.009727900554736456
88 85.06474896706641 0.051945077896118165 0.009582064439853032
89 85.22606957703829 0.05170606441497803 0.009530804961919784
90 85.21919760853052 0.051724512322743735 0.0095665107468764
91 85.30764321051538 0.0515091835975647 0.00949280787507693
92 85.39239194057882 0.05101719086964925 0.009336881518363952
93 85.32915592379868 0.05074697011311849 0.009369739393393198
94 85.39075267128646 0.050548714860280355 0.009314131279786428
95 85.38360920362175 0.05030852127075195 0.009284481952587763
96 85.32404448464513 0.05020628752708435 0.009305886377890905
97 85.37651559151709 0.050401920890808104 0.009563233097394307
98 85.40835719555616 0.04975984206199646 0.009269424716631572
99 85.25969913229346 0.049282926940917966 0.009379909912745158
100 85.52848288789392 0.04932776797612508 0.009259535223245621
101 85.32474591396749 0.04910694298744202 0.009163563519716263
102 85.20913532748818 0.048849842246373495 0.009216563999652863
103 85.21510664001107 0.04882265259424846 0.009259421080350876
104 85.3855206836015 0.048676239347457885 0.009118420203526816
105 85.31385610438883 0.04872555871009827 0.009062383006016413
106 85.26136815734208 0.04823111702601115 0.008901039471228918
107 85.18429894931614 0.048153520854314166 0.008846336116393407
108 85.03668233193457 0.0480907018661499 0.009242421368757884
109 84.96211854554713 0.047612708616256716 0.008912408341964086
110 84.88759677484632 0.047530815267562865 0.009068025300900142/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

111 84.76842264831066 0.04769499626159668 0.008886831591526667
112 84.86355908960104 0.047617550086975095 0.008888700932264328
113 84.82449276931584 0.04704921286900838 0.009015006939570108
114 84.76408072374761 0.04763283971150716 0.008917479008436204
115 84.7753071859479 0.047006051778793335 0.008765710314114888
116 84.84818990156054 0.04651499013900757 0.008983151117960612
117 84.74962222389877 0.04633049701054891 0.009166344493627548
118 84.72516046091914 0.04652125089963277 0.00891451354821523
119 84.83523187413812 0.04617500116030375 0.008605309774478277
120 84.96218323893845 0.046337851572036744 0.008578580766916276
121 84.85441114194691 0.04601579008102417 0.008710854748884837
122 84.89566686376929 0.045775877475738525 0.008814425488313038
123 85.08942195214331 0.04546807823181152 0.00870514127612114
124 85.51220715232193 0.04567552711963654 0.009145906070868175
125 85.56851445324719 0.04540590750376383 0.008494236061970393
126 85.52219827286899 0.045550111198425294 0.008435804069042206
127 85.38991264812648 0.04533969664573669 0.008442905614773432
128 85.29136880673468 0.045196628212928774 0.008544893473386765
129 85.17672528326511 0.04475551096598307 0.008456214239199957
130 85.41748178750277 0.04465459587574005 0.008372492909431458
131 85.44699219055474 0.04467586290041606 0.008347905029853185
132 85.46017652563751 0.044514957388242085 0.0083657366335392
133 85.37933790311217 0.044650005634625754 0.008763647854328156
134 85.28689319640398 0.04442153645356496 0.008301321506500245
135 85.34245408885181 0.04390315286318461 0.00828409343957901
136 85.4841150958091 0.05155584778785705 0.008287437548240026
137 85.45415494404733 0.04307560116449992 0.008250479658444722
138 85.40549845062196 0.04380534697373708 0.00830029724041621
139 85.3827307689935 0.043443354241053264 0.008296362042427063
140 85.6007413174957 0.043260870480537415 0.008038988312085469
141 85.47635897621512 0.043091063253084816 0.008122206528981527
142 85.45851400122046 0.043079592752456666 0.008120163530111312
143 85.31512002274394 0.043472138778368634 0.008115464051564535
144 84.97127662412822 0.04330044965744018 0.008369255185127258
145 84.72296838089824 0.04341562251249949 0.008333653599023818
146 84.68389342539012 0.042998906461397805 0.008149334530035655
147 84.8603545371443 0.04286558009783427 0.008036392629146577
148 84.86024002358317 0.04281622588634491 0.008299821108579635
149 84.76325627230108 0.04325181328455607 0.008186408042907715
150 84.8216765280813 0.0422876921415329 0.008151991804440817
151 84.7833341434598 0.04285560521284739 0.008087190161148707
152 84.79888175800443 0.04254018909136454 0.00801669602592786
153 84.7244325466454 0.042670598769187926 0.008042394946018855
154 84.7976234164089 0.04260668319861094 0.007832452038923899
155 84.673842638731 0.04207620314757029 0.008061273992061614
156 84.80043786764145 0.04236232208410899 0.008047538568576177
157 84.74682928808033 0.04212526998519898 0.007976806153853735
158 84.81649331748486 0.041783855891227725 0.007986882428328197
159 84.94815671443939 0.04193381601969401 0.007818112085262934
160 85.1571851465851 0.04188529170354208 0.00796865110596021
161 85.30572040006518 0.04192471404075623 0.008008486608664194
162 85.46059137023985 0.04158785475889842 0.00781382934252421
163 85.45029959082603 0.04149641489187876 0.007879381080468496
164 85.51143485307693 0.0414440572977066 0.007893169432878494
165 85.4099252242595 0.04165755596160889 0.007931110054254531
166 85.40330816060305 0.05792079441547394 0.007984749138355255
167 85.49271855130792 0.04070089294115702 0.007724959115187327
168 85.44067329913378 0.04056096555391948 0.007795834670464198
169 85.35050497576594 0.04080832693576813 0.007797068178653717
170 85.47402520105243 0.04081669330596924 0.007995565305153529
171 85.3810385838151 0.041010540747642515 0.007811512182156245
172 85.45278116501868 0.04097633247375488 0.008000038454929987
173 85.31563095375896 0.04112187894980113 0.007841794898112615
174 85.36341096274555 0.040933615136146545 0.0077353888352712
175 85.53626364283264 0.04070567565759023 0.007849969297647476
176 85.51938756927848 0.04047258617083232 0.00768763725956281
177 85.46591679379344 0.040550216555595396 0.008557315280040105
178 85.40955392830074 0.04129907259941101 0.007748383939266205
179 85.40168882533908 0.04039124919573466 0.007908883998791376
180 85.25156098045409 0.040442064150174456 0.007847700615723928
181 84.71643315255642 0.040494557603200275 0.007691661536693573
182 84.68981056846678 0.040419883720080055 0.007699610074361166
183 84.76588078029454 0.04005279878775279 0.007794054975112279
184 84.71525677293539 0.0401833221356074 0.007724962989489237
185 84.77671804651618 0.04013554759025574 0.00764181978503863
186 84.67849554121494 0.040314323465029396 0.0076809665064016975
187 84.7844026684761 0.04011391708056132 0.007995142747958501
188 84.77691715955734 0.03997302052974701 0.007574318637450536
189 84.68589253537357 0.039988023734092715 0.0076170376737912495
190 84.77006804943085 0.03986281487941742 0.007706702431042989
191 84.80137377604842 0.0397947363615036 0.0076703760127226515
192 84.84270377643406 0.03977693969408671 0.007559356729189555
193 84.73516614362597 0.03996870074272156 0.007836569637060165
194 84.88359412178397 0.03948082886536916 0.0074980622033278144
195 84.9879302661866 0.03965933838685354 0.007614735672871272
196 85.16198094189167 0.03955286591053009 0.007737468520800273
197 85.356217071414 0.039315799689292906 0.007852487335602442
198 85.28562092036009 0.03947965362071991 0.007581131587425868
199 85.46860524825752 0.03952194056510925 0.007507839838663737
Training Time:16946.136s
