nohup: ignoring input
begin to read data
Data Loading with 9.128 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─Tanh: 2-1                         --
│    └─ModuleList: 2-2                   --
│    │    └─Linear: 3-1                  384
│    │    └─Linear: 3-2                  16,512
│    │    └─Linear: 3-3                  16,512
│    │    └─Linear: 3-4                  528,384
│    └─ModuleList: 2-3                   --
│    │    └─BatchNorm1d: 3-5             256
│    │    └─BatchNorm1d: 3-6             256
│    │    └─BatchNorm1d: 3-7             256
│    │    └─BatchNorm1d: 3-8             8,192
├─FNO2d: 1-2                             --
│    └─GELU: 2-4                         --
│    └─Linear: 2-5                       64
│    └─ModuleList: 2-6                   --
│    │    └─SpectralConv2d: 3-9          663,552
│    └─ModuleList: 2-7                   --
│    │    └─Conv2d: 3-10                 1,056
│    └─Linear: 2-8                       4,224
│    └─Linear: 2-9                       516
=================================================================
Total params: 1,240,164
Trainable params: 1,240,164
Non-trainable params: 0
=================================================================
Training...
0 37.52841986902058 0.5492152324676514 0.05620129259427389
1 36.77683839946985 0.2842388325373332 0.044435339053471884
2 36.468139035627246 0.22351133861541747 0.03628545618057251
3 36.40652259811759 0.19448634243011476 0.03219735558827718
4 36.288403702899814 0.1740832526842753 0.029754300355911256
5 36.27143673785031 0.15976229467391967 0.02708265471458435
6 36.41892801783979 0.1473187827428182 0.025420398990313214
7 36.24699700437486 0.1382159540494283 0.024364169239997863
8 36.27100243605673 0.13085580937067667 0.02292123285929362
9 36.38683614693582 0.12391155808766682 0.021464538017908732
10 36.14930341206491 0.11864596290588379 0.020969948212305706
11 36.41243799030781 0.1143464314142863 0.020331511000792186
12 36.24138975888491 0.11061089919408162 0.019508541782697043
13 36.28783858381212 0.10711253906885783 0.01981532222032547
14 36.400356547906995 0.10461519587834676 0.019020239035288492
15 36.156338170170784 0.10127223517100016 0.017870092113812765
16 36.39196894876659 0.09910169086456298 0.01745517301559448
17 36.321173368021846 0.09707430651982625 0.01727960377931595
18 36.173969097435474 0.09449755516052247 0.016975787500540414
19 36.42043958418071 0.09286369438171387 0.016825759450594585
20 36.19851001910865 0.2181820002555847 0.0394322905143102
21 36.39179748669267 0.14159978351593017 0.019779301524162293
22 36.451950170099735 0.1021937977472941 0.017255953371524812
23 36.256796941161156 0.09464911543528239 0.017118279814720153
24 36.51089062541723 0.09096000919342041 0.01632461299498876
25 36.35437129624188 0.08885290877024332 0.01602884441614151
26 36.400502460077405 0.08729154715538025 0.015593774517377217
27 36.51802596822381 0.08666661265691121 0.015739107569058736
28 36.10896518826485 0.08520169774691264 0.015557692209879558
29 36.236087556928396 0.08512792604764302 0.015780210117499034
30 36.07040139846504 0.08382055622736613 0.015515601992607117
31 36.11875365488231 0.08270352881749471 0.015263033072153728
32 36.27189479582012 0.0818633823076884 0.015000543018182119
33 36.04893980547786 0.08174734182357789 0.015013601283232371
34 36.36046897619963 0.0798707057317098 0.014477331280708314
35 36.27607929520309 0.07969548484484355 0.01428387329975764
36 36.401593048125505 0.07848415093421936 0.014447453618049621
37 37.677973464131355 0.25838274141947426 0.026178914785385133
38 37.526750560849905 0.11641320273081461 0.017701149225234986
39 36.96619004197419 0.09247697588602702 0.015850701550642648
40 36.574740804731846 0.08558218668301901 0.015028896033763886
41 36.324705831706524 0.08181835657755533 0.01469580473502477
42 36.07823829166591 0.0803379951318105 0.014537253220876059
43 36.26012449711561 0.07968487327893575 0.014190206070741018
44 36.2077461425215 0.07862854475975037 0.014230152567227681
45 36.16734744608402 0.07683988780975341 0.014496572891871135
46 35.966503132134676 0.07704972399075825 0.013974499702453614
47 36.1816744171083 0.07627561442057292 0.013823805610338847
48 35.9453530870378 0.07525018843015036 0.014126999715964
49 36.09533396549523 0.0754140432993571 0.013604648888111115
50 36.139597807079554 0.07441990691820781 0.013684922595818837
51 35.92490562796593 0.21806579332351683 0.0209449432293574
52 36.15369410999119 0.09527681924502054 0.015257941822210947
53 35.92631024867296 0.08000369976361593 0.014406007587909699
54 36.10357150062919 0.07558921006520589 0.013800445973873138
55 36.12912300042808 0.07397222944895426 0.013568641126155852
56 35.89125782996416 0.07266722526550293 0.013643930713335674
57 36.160918245092034 0.07213067851066589 0.01317230902115504
58 35.95658396370709 0.0718081728776296 0.013230426251888276
59 36.06103652343154 0.07164800117810567 0.01309321035941442
60 36.13565453700721 0.07156649390856425 0.013009856005509694
61 35.90580428019166 0.07084862438837687 0.013428259789943695
62 36.157854622229934 0.07001945274670919 0.013206231772899629
63 35.97727941721678 0.07009595313072205 0.0130133842031161
64 36.074662048369646 0.06985728391011556 0.012831237812836965
65 36.1078182682395 0.06964648040135701 0.01343778916200002
66 35.91801072470844 0.06962164432207743 0.012579786558945974
67 36.172097938135266 0.06838533832232158 0.012656143168608348
68 35.98122654668987 0.06828996404012044 0.013102186898390452
69 36.02231923118234 0.06812297550837199 0.012838258743286132
70 36.17086764425039 0.0672968898932139 0.012511644721031188
71 35.90170282870531 0.0673721639474233 0.012524774114290873
72 36.14110323600471 0.13113833734194438 0.013570338249206544
73 35.98383618146181 0.06816256273587545 0.012200471103191377
74 36.033884570002556 0.06498070618311565 0.01217306399345398
75 36.13036747276783 0.06445517433484395 0.012101402461528778
76 35.89435965940356 0.06384670998255412 0.012177946587403616
77 36.150533616542816 0.06397317762374878 0.012079822381337483
78 35.99573729932308 0.06387469868659973 0.011958039959271749
79 36.03156243823469 0.0640998983224233 0.011966804563999177
80 36.127811178565025 0.06400377162297567 0.01232382196187973
81 35.84293637983501 0.06370466210047404 0.012252339879671732
82 36.16286099143326 0.06332849698066711 0.011786423544089
83 35.973208982497454 0.06302780664761862 0.011812842051188151
84 36.03310849145055 0.06292578260103862 0.011756777981917063
85 36.08843433856964 0.06295161178906758 0.01190392283598582
86 35.89882248453796 0.06252285084724427 0.011836306611696878
87 36.151933927088976 0.06282677097320556 0.011698107262452443
88 36.00158557854593 0.06181051754951477 0.011704197386900584
89 36.00518875941634 0.06171789886156718 0.011630729456742605
90 36.15997945331037 0.12883410453796387 0.02168339975674947
91 36.07332474552095 0.08244655202229818 0.01260381935040156
92 36.39757148362696 0.0645344930489858 0.011732727030913034
93 36.21461387723684 0.06140496242841085 0.011552299161752065
94 36.21546501852572 0.060228313795725505 0.011263547817866008
95 36.115227503702044 0.060080173603693644 0.011390456080436706
96 35.971455212682486 0.059789382266998294 0.011228056232134501
97 36.18893910571933 0.05980710078875224 0.011246626456578573
98 35.951591877266765 0.05971198360125224 0.011326272586981455
99 36.18912652507424 0.05932250221570333 0.011262686868508657
100 36.20259843952954 0.059263365538915 0.011308081090450287
101 36.000865099951625 0.05910083567301432 0.011133076171080272
102 36.205155396834016 0.059264692242940266 0.011200656394163767
103 36.00254295952618 0.059224337402979534 0.011286865631739298
104 36.19912623427808 0.05880499962170919 0.011224988202253977
105 36.215320190414786 0.05862117295265198 0.011135053118069966
106 35.99335549771786 0.058884572712580366 0.011247757991154989/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

107 36.266962978988886 0.05839965167045593 0.01104363332192103
108 36.034624207764864 0.057847363726298014 0.010976319203774135
109 36.21469081752002 0.058160554138819376 0.010902584423621495
110 36.216331608593464 0.08224715116818745 0.010722500512997309
111 36.0214695520699 0.055640683937072756 0.010796483059724173
112 36.293406531214714 0.05540462826093038 0.010676237503687541
113 36.09732688032091 0.05556990571022034 0.010647126207749049
114 36.180766532197595 0.056071359634399416 0.010814333011706671
115 36.26812610961497 0.05598359670639038 0.010539650628964106
116 36.023333417251706 0.0556798570950826 0.010734683314959208
117 36.30622128397226 0.055668894799550374 0.010595729808012645
118 36.08062248677015 0.05616622735659282 0.010846001942952474
119 36.19309244118631 0.055523131211598714 0.010636146048704782
120 36.29784931987524 0.05575663658777873 0.01057384310166041
121 36.032415479421616 0.05583929400444031 0.010645212968190512
122 36.29731516726315 0.05564403486251831 0.010602239539225896
123 36.05288305692375 0.08207511485417684 0.01057438095410665
124 36.29827637411654 0.05405968839327494 0.01030712647239367
125 36.24601715244353 0.053737149318059285 0.010329101781050364
126 36.0540556833148 0.053603016487757366 0.010554431627194086
127 36.33437608927488 0.05351469882329305 0.01022872977455457
128 36.07192979939282 0.05365430266062419 0.01027196321884791
129 36.24095769226551 0.054126589600245155 0.010479225665330888
130 36.24988638050854 0.05393018078804016 0.010293469051520029
131 36.05763273127377 0.05373831919034322 0.010249599317709605
132 36.32805978693068 0.053945130093892416 0.010269742558399837
133 36.116243148222566 0.05420962217648824 0.01037385259071986
134 36.19917945563793 0.05376531926790873 0.010293285300334295
135 36.301917776465416 0.0537550674756368 0.010202195584774018
136 36.0652120411396 0.053481628926595054 0.010485102564096451
137 36.29606542363763 0.0534026918888092 0.010277635425329208
138 36.09925800934434 0.053684890174865725 0.01014868900179863
139 36.16092735528946 0.07309519131978352 0.010064956188201904
140 36.227920757606626 0.05136634036699931 0.010213530510663987
141 36.02358349785209 0.051293396695454914 0.00989182272553444
142 36.269953943789005 0.05115339748064677 0.01000914407769839
143 36.13876798376441 0.05140132778485616 0.009974740892648697
144 36.145828204229474 0.051591856495539344 0.010009169360001883
145 36.276535799726844 0.05157947810490926 0.009973330795764923
146 35.979896400123835 0.051615642404556274 0.010044220755497615
147 36.27434701658785 0.05173130833307902 0.010049131631851196
148 36.058787206187844 0.0518641889890035 0.010322024901707968
149 36.174936236813664 0.051939164447784424 0.010002332389354706
150 36.22655365802348 0.05169232149124146 0.009965739369392395
151 35.98847360163927 0.05192884133656819 0.010104920516411464
152 36.27256786450744 0.05187961772282918 0.009701506614685058
153 36.06831827387214 0.05195515211423238 0.00990340283513069
154 36.11923557706177 0.051357630666097005 0.010185595750808716
155 36.227585760876536 0.05137315432230632 0.009744672884543736
156 35.98840774036944 0.05080962980588277 0.009868731737136842
157 36.25230740569532 0.05111460224787394 0.009865244815746943
158 36.06354639865458 0.051054170735677085 0.009772597163915634
159 36.134064331650734 0.09481589371363322 0.011842417279879253
160 36.242084136232734 0.05396622478167216 0.010076139032840728
161 35.97403850965202 0.049997248792648316 0.00970226197441419
162 36.23231390863657 0.04950545455614726 0.009575605193773906
163 35.996628530323505 0.04921000606218974 0.009630536297957102
164 36.17598840035498 0.04954811429977417 0.00971020288268725
165 36.14572564512491 0.04938621554374695 0.009727052400509516
166 35.91088729351759 0.04982147986094157 0.009639769305785497
167 36.16818059980869 0.04947287023862203 0.009592184404532115
168 35.955079859122634 0.0499107234954834 0.009774403552214305
169 36.10822610370815 0.04997753872871399 0.009538859556118648
170 36.12441679649055 0.04933449322382609 0.009458471238613129
171 35.90590860135853 0.04935912791887919 0.00954993016521136
172 36.17150941863656 0.04980695066452026 0.009648795346419017
173 35.964708875864744 0.04949027314186096 0.009590253353118897
174 36.08048370294273 0.04935567291577657 0.009484444518884022
175 36.14364998601377 0.049142186371485395 0.00948867921034495
176 36.3293343372643 0.049350080728530886 0.009769161115090052
177 36.14987787976861 0.049525176763534545 0.009620233337084453
178 36.14397158846259 0.07960225208600362 0.012772996147473653
179 35.99778809770942 0.051104988940556845 0.009381441503763199
180 36.045462826266885 0.047158726167678834 0.009410586049159368
181 36.11128930002451 0.04712240530649821 0.009210369020700455
182 35.883053816854954 0.04712299094200134 0.009179521550734837
183 36.15341507270932 0.047303883059819536 0.009422069688638051
184 35.92062994092703 0.04746574322382609 0.00928361541032791
185 36.08920122683048 0.04777849909464518 0.009379063963890075
186 36.08264736458659 0.04765797481536865 0.009498863180478413
187 35.906469425186515 0.047709337027867636 0.009343262890974681
188 36.16968342475593 0.04792519424756368 0.009316147923469544
189 35.92515833489597 0.04805217342376709 0.00958310106396675
190 36.09548778086901 0.04796904921531677 0.009665158490339915
191 36.0933576002717 0.04813302383422852 0.0094356489777565
192 35.9059493355453 0.047907176860173546 0.009408890455961227
Early stop at epoch 193
Training Time:7028.714s
