nohup: ignoring input
begin to read data
Data Loading with 8.542 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─Tanh: 2-1                         --
│    └─ModuleList: 2-2                   --
│    │    └─Linear: 3-1                  384
│    │    └─Linear: 3-2                  16,512
│    │    └─Linear: 3-3                  16,512
│    │    └─Linear: 3-4                  528,384
│    └─ModuleList: 2-3                   --
│    │    └─BatchNorm1d: 3-5             256
│    │    └─BatchNorm1d: 3-6             256
│    │    └─BatchNorm1d: 3-7             256
│    │    └─BatchNorm1d: 3-8             8,192
├─FNO2d: 1-2                             --
│    └─GELU: 2-4                         --
│    └─Linear: 2-5                       64
│    └─ModuleList: 2-6                   --
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    │    └─SpectralConv2d: 3-12         663,552
│    │    └─SpectralConv2d: 3-13         663,552
│    │    └─SpectralConv2d: 3-14         663,552
│    └─ModuleList: 2-7                   --
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    │    └─Conv2d: 3-18                 1,056
│    │    └─Conv2d: 3-19                 1,056
│    │    └─Conv2d: 3-20                 1,056
│    └─Linear: 2-8                       4,224
│    └─Linear: 2-9                       516
=================================================================
Total params: 4,563,204
Trainable params: 4,563,204
Non-trainable params: 0
=================================================================
Training...
0 31.46250551380217 1.005887713241577 0.103540358543396
1 30.580962309613824 0.40014780693054197 0.06020369291305542
2 30.682596964761615 0.3077036043167114 0.053892178535461424
3 30.72008310444653 0.26430410499572754 0.04583921432495117
4 30.742306642234325 0.23408921451568604 0.04269129991531372
5 30.73935996554792 0.2115114179611206 0.03854315161705017
6 30.782490948215127 0.19404995975494385 0.03432955026626587
7 30.798696503043175 0.18147983665466308 0.03245935559272766
8 30.84708123654127 0.1741152935028076 0.03198227047920227
9 30.852408381178975 0.16353687896728517 0.03019495725631714
10 30.91283417120576 0.15477761335372925 0.028479409217834473
11 30.88892614468932 0.1451901798248291 0.02679260730743408
12 30.871217116713524 0.13928371324539185 0.025590057373046874
13 30.920049430802464 0.13341458950042726 0.024915417432785036
14 30.863478671759367 0.12964967832565308 0.02323829531669617
15 30.91727582551539 0.12013942403793335 0.022516998052597045
16 30.841058699414134 0.11812556133270263 0.022596960067749024
17 30.926467947661877 0.11424275979995728 0.02166646480560303
18 30.867763249203563 0.11189993906021119 0.020776504278182985
19 30.94196898303926 0.10900531902313232 0.020173295140266417
20 30.892969388514757 0.10541795616149903 0.021650508642196656
21 30.971917876973748 0.10336338109970093 0.020308775901794432
22 30.895893160253763 0.10155454921722412 0.018988222479820252
23 30.904323937371373 0.09830375080108643 0.019277076125144958
24 30.90167194791138 0.09539705362319946 0.01941357374191284
25 30.917824804782867 0.09518382453918457 0.018711655735969543
26 30.865588016808033 0.09081859436035156 0.01771885633468628
27 30.887813948094845 0.08889236478805541 0.01817459046840668
28 30.87200770713389 0.08711724729537963 0.016908491253852843
29 30.92146966047585 0.0841802634716034 0.016908085346221922
30 30.905241180211306 0.08477549138069153 0.017123410105705263
31 30.920286497101188 0.0846282166004181 0.016915748715400695
32 30.846930963918567 0.08226703252792358 0.016227046251296996
33 30.939534157514572 0.08068963012695313 0.016245110034942625
34 30.878939686343074 0.07895987758636475 0.016322627067565917
35 30.888611678034067 0.07763643169403076 0.015821498036384583
36 30.867667235434055 0.07637683463096619 0.015421000123023988
37 30.86427509225905 0.07774080424308777 0.015540589094161988
38 30.8735540676862 0.07268520874977112 0.015301352143287658
39 30.841321339830756 0.0729286916255951 0.015518230199813843
40 30.92330882139504 0.07188950872421265 0.0147111576795578
41 30.808925293385983 0.07128328304290771 0.015224517583847045
42 30.879287615418434 0.07217024502754212 0.01435625672340393
43 30.80911811441183 0.07008739223480225 0.01479400634765625
44 30.8830302413553 0.0675456705570221 0.014214216470718384
45 30.7863380163908 0.06892336821556092 0.01460328757762909
46 30.85648257471621 0.06775865921974182 0.014132023453712464
47 30.817677838727832 0.0657179675102234 0.013416739702224732
48 30.833888167515397 0.06461245951652526 0.013768177032470703
49 30.832497650757432 0.06382705426216126 0.013748139142990112
50 30.838664511218667 0.06277391757965088 0.014254345893859863
51 30.80800335109234 0.4220551503181458 0.025617296695709228
52 30.785407084971666 0.10928146314620972 0.01791759133338928
53 30.817077757790685 0.08309238314628602 0.016011221408843992
54 30.78425240330398 0.07393418684005737 0.01494284451007843
55 30.820858234539628 0.0713359013080597 0.01456851840019226
56 30.750748332589865 0.0705105375289917 0.014452807307243347
57 30.818656185641885 0.06769311399459839 0.013769103288650513
58 30.734039088711143 0.0670728310585022 0.014585787653923035
59 30.84218973107636 0.06798948125839234 0.013579782247543335
60 30.779382597655058 0.06368202872276306 0.013646854162216187
61 30.843387063592672 0.0623619083404541 0.013341748714447021
62 30.811500692740083 0.062378717851638794 0.013296399116516113
63 30.824843978509307 0.0632851270198822 0.013235586285591126
64 30.828079169616103 0.06031240463256836 0.013233149647712708
65 30.814712900668383 0.061356810569763186 0.012976171970367432
66 30.83175083808601 0.05951766333580017 0.012860610485076904
67 30.773173751309514 0.06024770712852478 0.013101438879966737
68 30.799669589847326 0.06100159592628479 0.012973955869674682
69 30.7646624147892 0.061253897428512574 0.013366931080818177
70 30.864493239670992 0.05890742340087891 0.01259242057800293
71 30.7888851352036 0.05771097526550293 0.013079819083213807
72 30.841086633503437 0.05767572917938232 0.012840901613235473
73 30.7546283043921 0.05703450565338135 0.012952816486358643
74 30.871828908100724 0.05667498393058777 0.01243329405784607
75 30.812710693106055 0.05497622199058533 0.012289729714393616
76 30.815029272809625 0.05420318555831909 0.013020713925361633
77 30.817201191559434 0.05684857120513916 0.012254074215888977
78 30.810076473280787 0.05619900321960449 0.01263145089149475
79 30.80447950027883 0.05398795318603516 0.012257370948791504
80 30.898348415270448 0.05289093017578125 0.011896230578422547
81 30.826959796249866 0.05316796069145203 0.012267685532569884
82 30.78401185385883 0.05481245937347412 0.012208449244499207
83 30.821281425654888 0.052358666515350344 0.01217821478843689
84 30.797419413924217 0.19292127633094788 0.04608091354370117
85 30.79497371427715 0.10267554106712341 0.012687659859657287
86 30.845297185704112 0.05121213507652283 0.011582396030426025
87 30.822463415563107 0.050571541118621825 0.012355718612670898
88 30.851788828149438 0.04960062694549561 0.011466276049613953
89 30.885172490030527 0.04854608521461487 0.011203318238258361
90 30.83343779295683 0.04694168057441712 0.012069371342658997
91 30.87740269675851 0.04906245470046997 0.011478847861289977
92 30.84379825927317 0.04610701670646668 0.011226794719696044
93 30.862254517152905 0.04853061299324036 0.01138644278049469
94 30.807876631617546 0.04956334104537964 0.011378839612007141
95 30.889800284057856 0.04751304364204407 0.011886183619499207
96 30.851157108321786 0.048106231737136844 0.011375592350959777
97 30.909819424152374 0.04594707131385803/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
 0.01099535584449768
98 30.836830420419574 0.04723967061042786 0.011723456978797913
99 30.843818617984653 0.04811925458908081 0.011193644404411316
100 30.87769613415003 0.04621476655006409 0.011457841992378235
101 30.882946096360683 0.04861609754562378 0.011473594307899475
102 30.887540630996227 0.04859125289916992 0.01183760941028595
103 30.87211762741208 0.0484998779296875 0.011313753724098206
104 30.895351557061076 0.046535687255859375 0.011017004251480103
105 30.849468296393752 0.04667732772827148 0.011511395573616029
106 30.926392329856753 0.04972312779426575 0.011251083612442016
107 30.874139718711376 0.04619989976882934 0.011188835501670838
Early stop at epoch 108
Training Time:3372.500s
