nohup: ignoring input
begin to read data
Data Loading with 9.238 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─SiLU: 2-1                         --
│    └─KAN: 2-2                          --
│    │    └─GELU: 3-1                    --
│    │    └─ParameterList: 3-2           1,049,088
│    │    └─ParameterList: 3-3           8,392,704
│    │    └─ModuleList: 3-4              8,704
│    │    └─ModuleList: 3-5              2
├─FNO2d: 1-2                             --
│    └─GELU: 2-3                         --
│    └─Linear: 2-4                       64
│    └─ModuleList: 2-5                   --
│    │    └─SpectralConv2d: 3-6          663,552
│    │    └─SpectralConv2d: 3-7          663,552
│    │    └─SpectralConv2d: 3-8          663,552
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    └─ModuleList: 2-6                   --
│    │    └─Conv2d: 3-12                 1,056
│    │    └─Conv2d: 3-13                 1,056
│    │    └─Conv2d: 3-14                 1,056
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    └─Linear: 2-7                       4,224
│    └─Linear: 2-8                       516
=================================================================
Total params: 13,442,950
Trainable params: 13,442,950
Non-trainable params: 0
=================================================================
Training...
0 144.59734226763248 2.235296137237549 0.04666617155075073
1 143.39809903129935 0.26518404388427735 0.03840605616569519
2 142.46332699805498 0.20049444863001506 0.029902374744415282
3 142.12469231709838 0.1518499563852946 0.023797394037246705
4 142.2835975345224 0.1250252356529236 0.019704474210739134
5 142.11891694366932 0.10651549523671468 0.017988266944885253
6 142.2130584847182 0.09004734145800272 0.015955379605293273
7 142.15601157210767 0.08123582201004029 0.0145029616355896
8 142.27181180007756 0.07454674660364786 0.012416630387306213
9 142.1781056765467 0.06818206534385682 0.012036855220794678
10 142.39187482744455 0.06412989511489868 0.012802175879478454
11 142.2017805352807 0.060226136048634846 0.011341700553894043
12 142.30062627233565 0.05696063505808512 0.010368512272834777
13 142.23513707891107 0.05406162740389506 0.009893999993801117
14 142.29183935560286 0.05396014507611593 0.009372223019599914
15 142.33903359994292 0.04998429732322693 0.009451388418674468
16 142.20575597323477 0.04851038009325663 0.009122653305530548
17 142.26568302698433 0.047092574723561605 0.008948688805103302
18 142.17406196147203 0.046472568615277605 0.008463167250156403
19 142.25916201248765 0.0446666086991628 0.00857510894536972
20 142.23994861356914 0.043293177247047426 0.008169003129005432
21 142.2194615751505 0.041156543310483296 0.007978124022483825
22 142.09918486326933 0.04072078924973806 0.007950852811336517
23 142.15168198384345 0.039868335843086244 0.007991885244846344
24 142.1085661444813 0.03891150515874227 0.007750802040100098
25 142.219112213701 0.03718274269104004 0.0077443444728851315
26 142.05889067053795 0.037591439882914225 0.007457779943943024
27 142.11719000712037 0.03550818529129028 0.00803871750831604
28 142.04997447505593 0.03567642469406128 0.007318477332592011
29 142.19882756471634 0.03437028229236603 0.0070896512269973755
30 142.23151136934757 0.03322707773844401 0.00683508038520813
31 142.25995993986726 0.03372290112972259 0.00841028094291687
32 142.2120706550777 0.03385849586327871 0.006532644629478454
33 142.22860958240926 0.031947999008496604 0.0066660428047180175
34 142.26583506725729 0.032160856000582376 0.006725081503391266
35 142.38675343617797 0.03003587074279785 0.006454115509986877
36 142.25056896358728 0.030367018683751423 0.006056116819381714
37 142.39766027405858 0.03013504628340403 0.006029263734817505
38 142.32941201515496 0.02921557741165161 0.006280403435230255
39 142.39659362472594 0.029010354709625243 0.006385408639907837
40 142.43340913951397 0.027753893558184306 0.006011598110198975
41 142.44875023514032 0.027528334283828736 0.006163051128387451
42 142.3710186649114 0.028806600618362425 0.005592944324016571
43 142.44872435368598 0.026671437295277912 0.005966847538948059
44 142.43082864396274 0.02560160616238912 0.005412313938140869
45 142.2915177680552 0.026624842302004497 0.005662870407104492
46 142.3878680486232 0.02603260633945465 0.005540199875831604
47 142.20113794319332 0.02608434925079346 0.005412324070930481
48 142.2513386476785 0.02507408351103465 0.0053824886679649355
49 141.22298881039023 0.024502385679880778 0.005176323056221008
50 142.7497346661985 0.02407734929720561 0.005312118530273437
51 143.4819147400558 0.02385450719197591 0.005761179625988006
52 143.64061134122312 0.02434269188642502 0.005498489141464233
53 143.5989406146109 0.024122207804520925 0.005128557384014129
54 143.7208366189152 0.024547987842559814 0.005007641613483429
55 143.634149922058 0.02389330801566442 0.00536623477935791
56 143.76216497644782 0.022931435283025107 0.005024511814117432
57 143.58883964829147 0.022850655126571657 0.00496680736541748
58 143.88717292435467 0.02252739858229955 0.005018391013145447
59 143.57143982313573 0.022724310028553008 0.0050465667247772215
60 143.56392663158476 0.02214704273144404 0.00556305080652237
61 143.6582091152668 0.022377759798367817 0.004901848584413528
62 143.5588862579316 0.021061747785409292 0.00489986315369606
63 143.51891009137034 0.021677814467748007 0.004993998110294342
64 143.35053147003055 0.020849891571203868 0.005050768703222274
65 143.57617821916938 0.02138041529258092 0.004779383391141891
66 143.37171549908817 0.021103629509607952 0.004902419596910477
67 143.40264740400016 0.020425901532173156 0.004983809143304825
68 143.51356880366802 0.022612612851460774 0.004609593152999878
69 143.52622403390706 0.020038043133417765 0.004850685894489289
70 143.4113243483007 0.02015459983746211 0.004726879149675369
71 143.42405808903277 0.020453462398052215 0.005079583823680877
72 143.3272001184523 0.01963927529255549 0.004961844980716706
73 143.49465090222657 0.02116815560658773 0.004876315593719482
74 143.4302692655474 0.0197481599966685 0.004430960416793823
75 143.4865533877164 0.019041784342130025 0.004574209600687027
76 143.38378967531025 0.020169708307584126 0.004620513916015625
77 143.47234393656254 0.01942000777324041 0.004552710503339767
78 143.4288748782128 0.01943832618792852 0.0043752133846282956
79 143.69586588069797 0.018827286005020143 0.004615181684494018
80 144.16192174516618 0.02019879500468572 0.00448257178068161
81 143.51678308472037 0.018814404702186583 0.004479461014270782
82 143.53226242028177 0.01857865983645121 0.004366806149482727
83 143.5019527040422 0.018791772719224293 0.00454656332731247
84 143.43142190389335 0.018608453691005708 0.005004431307315827
85 143.50494949519634 0.01915038718779882 0.005001738965511322
86 143.43483759649098 0.018551446036497752 0.004497565478086472
87 143.68441912718117 0.017837591846783955 0.004340128302574158
88 143.5377415716648 0.018066648264726003 0.0042776912450790405
89 143.5273231640458 0.018244298207759857 0.0043105496466159824
90 143.51054749637842 0.017640324699878692 0.004921571761369706
91 143.5112215578556 0.017881168778737386 0.004335561096668244
92 143.46710184775293 0.01800599836111069 0.004336945563554764
93 143.5150697659701 0.017975087575117746 0.00428786501288414
94 143.5239789094776 0.017450381457805635 0.004290567040443421
95 143.63286503963172 0.018028257115681966 0.004259526580572128
96 143.60335710830986 0.01668529993693034 0.0041107690334320065
97 143.60780067369342 0.01763605602582296 0.0042061744630336765
98 143.81970592960715 0.017095035656293233 0.00406926691532135
99 143.60000585019588 0.016711739071210225/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
 0.00460459977388382
100 143.7094078399241 0.017008662243684133 0.00419208437204361
101 143.9796361029148 0.016931718389193218 0.004139326959848404
102 143.6336819138378 0.017152641240755717 0.00417211577296257
103 143.47704758495092 0.01667957853078842 0.0042148658633232115
104 143.49534719623625 0.017165993289152782 0.004152873754501343
105 143.80999853275716 0.01741988652149836 0.004165254384279251
106 143.57571231387556 0.01656573471625646 0.004484303295612335
107 143.95419761165977 0.016628785228729247 0.004022208750247955
108 143.94714051671326 0.016617928099632265 0.003990946859121322
109 143.62242710217834 0.016910741392771402 0.004107597172260284
110 143.6544207353145 0.01632082962989807 0.004076018929481506
111 143.97255693748593 0.016064103241761527 0.0040045568346977235
112 144.03961830027401 0.015742379474639894 0.00406154602766037
113 144.0097278561443 0.01630607575972875 0.003950808197259903
114 143.64118472859263 0.015997079996267954 0.0045261338353157045
115 143.83816501498222 0.01574908247391383 0.004004310518503189
116 144.4243724923581 0.016238933285077414 0.0042858818173408504
117 143.69486156478524 0.015579566009839375 0.004028826355934143
118 143.86617761850357 0.015540424303213755 0.004030246734619141
119 143.8711524810642 0.015893426156044006 0.003973611742258072
120 144.37394693680108 0.015848416447639464 0.003943321108818054
121 143.9227611552924 0.015401893246173859 0.004253726601600647
122 144.0750665999949 0.015345928835868836 0.004064181447029113
123 143.94744047522545 0.015201843861738841 0.0038971738517284394
124 144.30097636952996 0.015217848904927571 0.0039057131111621857
125 144.37966810166836 0.015496138620376586 0.0038172762095928193
126 143.6828337609768 0.014924554737408956 0.004098425507545471
127 143.9983929283917 0.015214856779575348 0.003781846612691879
128 143.62338732928038 0.015308167326450349 0.004226474463939667
129 143.58676232397556 0.015076978619893391 0.003935277312994003
130 143.86193888634443 0.0153206502755483 0.004171450287103653
131 143.84839654155076 0.015648248640696208 0.0038796211779117584
132 144.26948164775968 0.014964624965190888 0.003972490131855011
133 144.35046218335629 0.014879536775747935 0.003966048657894135
134 144.33050693199039 0.015206558652718862 0.004064943194389343
135 143.68736987933517 0.015582940248648326 0.003912673741579056
136 143.85091377981007 0.014776768231391907 0.0037392504513263703
137 143.65149255655706 0.014478894591331481 0.004022142142057419
138 143.53244570083916 0.014342795821030934 0.003919954299926758
139 143.74199488013983 0.015580151768525442 0.003833853006362915
140 143.7127557322383 0.014549487094084422 0.003881418853998184
141 143.78783325478435 0.01490284130970637 0.003684658408164978
142 143.78691333718598 0.014095935169855753 0.0037245629727840424
143 143.7133013010025 0.014474949808915456 0.0037452688813209532
144 143.41424008645117 0.014133389337857565 0.0037727439403533935
145 143.74757470563054 0.01433387146393458 0.0038664551079273224
146 143.76897111162543 0.014725461264451345 0.003732648491859436
147 143.5383246652782 0.01418325236638387 0.003685573488473892
148 143.60046495124698 0.01388186004559199 0.003655441403388977
149 143.4518454708159 0.014399666233857474 0.003711254745721817
150 143.85760464705527 0.01521607942978541 0.003956740200519562
151 143.5325563289225 0.014170387959480286 0.0037614768743515015
152 143.53417289443314 0.013788329184055328 0.0036966681480407717
153 143.44246846437454 0.013763871729373931 0.004124449044466019
154 143.57394284568727 0.014004149055480957 0.0037897905707359315
155 143.46863760054111 0.014018317206700643 0.003960134387016296
156 143.52373321726918 0.01395251948038737 0.00372381255030632
157 143.443826880306 0.013990019166469574 0.0038407059013843537
158 143.5358917042613 0.013897315454483031 0.0037499427795410155
Early stop at epoch 159
Training Time:22930.117s
