nohup: ignoring input
begin to read data
Data Loading with 8.413 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─SiLU: 2-1                         --
│    └─KAN: 2-2                          --
│    │    └─GELU: 3-1                    --
│    │    └─ParameterList: 3-2           1,049,088
│    │    └─ParameterList: 3-3           8,392,704
│    │    └─ModuleList: 3-4              8,704
│    │    └─ModuleList: 3-5              2
├─FNO2d: 1-2                             --
│    └─GELU: 2-3                         --
│    └─Linear: 2-4                       64
│    └─ModuleList: 2-5                   --
│    │    └─SpectralConv2d: 3-6          663,552
│    │    └─SpectralConv2d: 3-7          663,552
│    │    └─SpectralConv2d: 3-8          663,552
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    └─ModuleList: 2-6                   --
│    │    └─Conv2d: 3-12                 1,056
│    │    └─Conv2d: 3-13                 1,056
│    │    └─Conv2d: 3-14                 1,056
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    └─Linear: 2-7                       4,224
│    └─Linear: 2-8                       516
=================================================================
Total params: 13,442,950
Trainable params: 13,442,950
Non-trainable params: 0
=================================================================
Training...
0 89.66798852570355 2.235356665102641 0.04654460668563843
1 88.75216377712786 0.27094119065602623 0.0341558301448822
2 88.54295803233981 0.20240451997121175 0.03041204810142517
3 88.45316294953227 0.16416522474288942 0.02294320344924927
4 88.4384848587215 0.12666077286402386 0.02088941216468811
5 88.47638979740441 0.10804196548461914 0.016861612200736998
6 88.48611542023718 0.09274953896204631 0.015387827157974243
7 88.45653268322349 0.08452116468747457 0.0149811851978302
8 88.430884052068 0.07689459781646729 0.013392230868339539
9 88.41647233255208 0.07075676639874776 0.012566140294075013
10 88.5084555670619 0.06654647634824117 0.01276236653327942
11 88.45052111335099 0.06395280688603719 0.011039706468582154
12 88.4729185923934 0.05807604773839315 0.01098044216632843
13 88.47698999755085 0.055561725012461344 0.010181313753128052
14 88.44865525886416 0.054582070255279544 0.009608988165855408
15 88.48902950249612 0.05206225617726644 0.009522373080253601
16 88.49193905293941 0.05012696148554484 0.009518199265003205
17 88.51606405340135 0.04814570951461792 0.009054308533668518
18 88.52870862744749 0.04688767893314361 0.009713888168334961
19 88.51094971597195 0.045521066705385846 0.008549732267856597
20 88.59680256992579 0.045037957564989725 0.008449947834014893
21 88.48598419688642 0.04273358057339986 0.008253608345985413
22 88.50155199877918 0.0405749849319458 0.008097503781318665
23 88.48664455860853 0.04050771996180216 0.00796332061290741
24 88.52389288693666 0.04012131765683492 0.008037548661231995
25 88.53679808415473 0.03946934990088145 0.00849226862192154
26 88.48062828928232 0.041111861594518025 0.007481809556484222
27 88.47771856375039 0.03629936159451803 0.008125022053718567
28 88.46740584261715 0.03658340439001719 0.007021895051002503
29 88.44746512360871 0.03442604313691457 0.0075754612684249874
30 88.53109477646649 0.03426069126923879 0.00709511786699295
31 88.46372622996569 0.033647315208117166 0.0068013396859169005
32 88.42438952624798 0.033326449592908224 0.007003882825374603
33 88.42998100444674 0.033592911028862 0.006404910981655121
34 88.43679120391607 0.03154461292425791 0.006363792717456818
35 88.42450444959104 0.031903106427192685 0.006784820556640625
36 88.40172122605145 0.0305591459274292 0.006167682111263275
37 88.45287551172078 0.030454370792706808 0.00600138396024704
38 88.39842765778303 0.03066963121096293 0.006244222521781921
39 88.43255675956607 0.029806008545557656 0.008099125623703003
40 88.49529321491718 0.030199920813242593 0.0062045127153396605
41 88.40368922799826 0.02790906221071879 0.005720188319683075
42 88.36430858448148 0.027848661637306215 0.005737688541412354
43 88.3945661727339 0.027324466784795127 0.006081734895706177
44 88.43175077810884 0.027923680679003398 0.0060228464007377625
45 88.39153792150319 0.027043269817034404 0.0060845580697059636
46 88.40521921776235 0.02657559938430786 0.005557328760623932
47 88.4077323321253 0.025590537985165914 0.0056583589315414426
48 88.40943111293018 0.025914046176274616 0.005430337488651276
49 88.47434332966805 0.025440102219581606 0.005513885319232941
50 88.58487322740257 0.0253200474580129 0.005678224563598633
51 88.4624172244221 0.025168780819574993 0.005497311055660248
52 88.48334618099034 0.024407167736689248 0.005675759017467499
53 88.50133133679628 0.02407328095436096 0.0055793547630310055
54 88.49684677645564 0.023355834968884787 0.0056488236784935
55 88.50040657632053 0.024727646470069885 0.005291060507297516
56 88.5850237403065 0.023946081042289735 0.005321861505508423
57 88.49937514029443 0.022896578256289164 0.00534599781036377
58 88.51431101933122 0.0228519890944163 0.0051782956719398495
59 88.56201958656311 0.0228234028895696 0.0054720711708068845
60 88.63564005494118 0.023040049870808918 0.0058618104457855225
61 88.64177501574159 0.022911799887816112 0.005032023191452026
62 88.59279746562243 0.022223879194259645 0.005080900192260742
63 88.60020933672786 0.02243222591082255 0.005064647048711777
64 88.57190690748394 0.020854197907447816 0.005293056666851044
65 88.53972523100674 0.0212526216228803 0.005161985158920288
66 88.6552784461528 0.021489955989519756 0.00493662878870964
67 88.54826349392533 0.020977373309930165 0.005102051347494125
68 88.5594332087785 0.021680604271094004 0.005116354823112488
69 88.60404035076499 0.020699176092942556 0.005155293047428131
70 88.75402310490608 0.020243140232563017 0.004717334806919098
71 88.55370318889618 0.020346225333213806 0.005086661577224732
72 88.57483207061887 0.020370489581425986 0.0048249375820159915
73 88.52740322612226 0.020653872136274974 0.004937693923711777
74 88.51574799977243 0.020066582743326823 0.004879676550626755
75 88.65330848097801 0.019444633837540946 0.004645877331495285
76 88.51430637016892 0.019869742651780445 0.004899643957614898
77 88.52515646070242 0.019768391132354738 0.004915371835231781
78 89.01900498569012 0.02050948205391566 0.00459467351436615
79 89.91045932471752 0.01919552622238795 0.004611106365919113
80 90.91404802724719 0.018991148503621418 0.004550002217292785
81 91.2691844869405 0.019087237079938254 0.004786368608474732
82 91.45919778570533 0.018875384692351022 0.004920882880687713
83 91.48567846417427 0.018256141356627148 0.004683820158243179
84 91.50307104736567 0.018838567876815798 0.004831307530403137
85 91.52747541107237 0.018921711643536884 0.004656476825475693
86 91.51528098620474 0.018423555394013722 0.004639275819063187
87 91.65365865454078 0.01885532255967458 0.004531162232160569
88 91.54401700384915 0.019563128145535787 0.005219215452671051
89 91.60783428326249 0.017887762264410655 0.004461931735277176
90 91.65288392826915 0.017978760957717894 0.005565230548381805
91 91.52123242244124 0.01786324233611425 0.004658330827951431
92 91.65798516757786 0.017841973785559337 0.004456859678030014
93 91.53496689535677 0.01744863996108373 0.004518018364906311
94 91.44415229558945 0.017699662359555564 0.004577314108610153
95 91.46634266711771 0.01724331170717875 0.005264731347560883
96 91.38664736784995 0.017426075049241384 0.004753662049770355
97 91.43858116865158 0.01759015504916509 0.004863271713256836
98 91.40977786667645 0.01768463397026062 0.004578491747379303
99 91.37034737318754 0.016777120792865752 0.004535508006811142
100 91.58512314595282 0.01674778855641683/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
 0.004406766295433045
101 91.468261314556 0.016673326905568442 0.00432713732123375
102 91.34990286268294 0.017524583367506664 0.004538753479719162
103 91.40139437280595 0.01733864993651708 0.0043975901603698735
104 91.31265675462782 0.016560691161950428 0.004413002729415893
105 91.35103783011436 0.016740062594413758 0.004519426375627518
106 91.31170859187841 0.01631564969221751 0.004529407471418381
107 91.27856778912246 0.01655607534646988 0.00445242777466774
108 91.34838407300413 0.01656651314496994 0.004521556794643402
109 91.47078084014356 0.015916353587309518 0.004317724406719208
110 91.44326663017273 0.015945884931087492 0.00446543350815773
111 91.31126328743994 0.016256767110029855 0.004396486282348633
112 91.33149364031851 0.01661220740477244 0.004382333308458328
113 91.33953055366874 0.016563552323977153 0.004427465051412583
114 91.39890433847904 0.016411769874890646 0.004467843770980835
115 91.4087859801948 0.015921348945299785 0.004267629534006119
116 91.316451061517 0.015999515430132547 0.004430330097675323
117 91.43484659679234 0.016096788223584492 0.00418977215886116
118 91.28641219995916 0.01541232574780782 0.004226302057504654
119 91.30486031062901 0.015429333098729452 0.00439418077468872
120 91.45056563615799 0.015845915377140044 0.004325547218322754
121 91.32885227911174 0.01588313432534536 0.00450913667678833
122 91.33812071941793 0.015438038583596547 0.004385460019111633
123 91.32936212420464 0.015608837719758352 0.004514136761426926
124 91.35675551369786 0.015721326184272764 0.004383677244186401
125 91.40134891681373 0.01517058200041453 0.004471571743488311
126 91.36398802697659 0.015133828004201252 0.004243700504302978
127 91.32457040436566 0.015760053980350495 0.004331312030553818
128 91.45136717893183 0.01549340038696925 0.004145426303148269
129 91.2770373467356 0.014657734874884287 0.004150847494602203
130 91.46097003854811 0.015061986056963603 0.004156018644571304
131 91.37817579135299 0.016652559280395506 0.004229837656021118
132 91.35415578447282 0.015434206211566924 0.004218364655971527
133 91.35184729471803 0.014784583405653636 0.004513626843690872
134 91.30340367741883 0.015392196659247081 0.004334588050842286
135 91.38659619353712 0.014561959640185038 0.00422613263130188
136 91.42009172216058 0.014385843380292257 0.0042668402194976805
137 91.44151917099953 0.014843160001436869 0.004116446226835251
138 91.47790610417724 0.014246539676189423 0.004061075001955032
139 91.37769464403391 0.014890072027842203 0.004115104377269745
140 91.51196713000536 0.014621949541568756 0.004221354871988297
141 91.37237327732146 0.014774088784058888 0.004365199655294419
142 91.37357176467776 0.014538423363367716 0.004462998956441879
143 91.38793427497149 0.014236826006571451 0.004323114007711411
144 91.36756951548159 0.014174276077747345 0.004271681904792785
145 91.3579750135541 0.014622868831952412 0.004186541289091111
146 91.45964758656919 0.014254248762130738 0.00410659596323967
147 91.409334782511 0.014178879284858704 0.004451218694448471
148 91.37565907649696 0.014052711908022562 0.004203999638557434
Early stop at epoch 149
Training Time:13491.605s
