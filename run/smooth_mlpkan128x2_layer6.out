nohup: ignoring input
begin to read data
Data Loading with 9.396 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─SiLU: 2-1                         --
│    └─KAN: 2-2                          --
│    │    └─GELU: 3-1                    --
│    │    └─ParameterList: 3-2           540,928
│    │    └─ParameterList: 3-3           4,327,424
│    │    └─ModuleList: 3-4              8,704
│    │    └─ModuleList: 3-5              3
├─FNO2d: 1-2                             --
│    └─GELU: 2-3                         --
│    └─Linear: 2-4                       64
│    └─ModuleList: 2-5                   --
│    │    └─SpectralConv2d: 3-6          663,552
│    │    └─SpectralConv2d: 3-7          663,552
│    │    └─SpectralConv2d: 3-8          663,552
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    └─ModuleList: 2-6                   --
│    │    └─Conv2d: 3-12                 1,056
│    │    └─Conv2d: 3-13                 1,056
│    │    └─Conv2d: 3-14                 1,056
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    └─Linear: 2-7                       4,224
│    └─Linear: 2-8                       516
=================================================================
Total params: 8,869,511
Trainable params: 8,869,511
Non-trainable params: 0
=================================================================
Training...
0 133.5401335582137 1.1840189715703329 0.0553559132417043
1 135.74146538041532 0.24859930655161538 0.033171331922213235
2 136.81809376738966 0.1822664605140686 0.024495890657107035
3 136.83882476761937 0.134642156791687 0.020909746825695037
4 136.72714376822114 0.11209789021809896 0.01651326584815979
5 136.63375783711672 0.09016926528612773 0.014940245091915131
6 136.81587929278612 0.08059735711415608 0.013853892465432486
7 136.82357972674072 0.07063736815452576 0.011948102116584779
8 137.29969830065966 0.06781178533236186 0.011727748493353526
9 137.58389972895384 0.06253709564208984 0.010955673972765605
10 137.70357742905617 0.05952731523513794 0.009973365147908529
11 137.58751624822617 0.05402766637802124 0.009477268517017365
12 137.6562770474702 0.053245451895395916 0.010030484318733216
13 137.63600726425648 0.05038653523127238 0.008468032479286194
14 137.57832756265998 0.04906510767141978 0.00852466556429863
15 137.6663045939058 0.046057413109143575 0.008593572477499645
16 137.56050061061978 0.047523756043116254 0.008338784327109655
17 137.62520540319383 0.046128423484166466 0.007942164490620296
18 137.63675069063902 0.04190675706068675 0.007830189317464828
19 137.53185282833874 0.041103326082229615 0.007051359514395396
20 137.0128387324512 0.04045411067008972 0.007738917052745819
21 136.66647399961948 0.03975368078549703 0.006912610977888107
22 136.7327505685389 0.039429398504892986 0.007011516769727071
23 136.68966520763934 0.038465819176038106 0.007093423157930374
24 136.74448797293007 0.0363247597138087 0.006818402498960495
25 136.80899226106703 0.03543917712370555 0.006830386469761531
26 136.87222927995026 0.03569361781279246 0.0069162022570768995
27 137.13545228727162 0.034800541130701704 0.006524619777997335
28 137.447826648131 0.03381252111593882 0.006802591850360235
29 137.6707183793187 0.03268505481084188 0.006210651139418284
30 137.60325449705124 0.0329609219789505 0.00610287007689476
31 137.6237328927964 0.032714390436808266 0.005985708465178808
32 137.5913206320256 0.031411104353268944 0.006550518969694773
33 137.63353811576962 0.03147141134738922 0.006565676271915436
34 137.60583773627877 0.0311657528479894 0.006438275019327799
35 137.54913985356688 0.029600072717666626 0.005666530499855677
36 137.61649802327156 0.030605808957417806 0.005639375547568003
37 137.49766995944083 0.0288158388932546 0.005360742290814718
38 137.16238715872169 0.02885631357828776 0.005806839545567831
39 136.64251612499356 0.028006930200258892 0.005285618936022122
40 136.83583129756153 0.02821302233537038 0.005558736314376195
41 136.6758834850043 0.027949053613344828 0.005951448371013006
42 136.6677935551852 0.027474362428983053 0.005682462056477864
43 136.78710576333106 0.026202138344446818 0.005558521151542663
44 136.81062921509147 0.028059878794352212 0.005171630228559176
45 137.0522262249142 0.02572822953859965 0.005623877286911011
46 137.23654197901487 0.025556014959017437 0.005016184096535047
47 137.65249767154455 0.02635691430568695 0.00511104820171992
48 137.5352704729885 0.02528077147801717 0.006477068195740382
49 137.59479202330112 0.0248262042204539 0.0052601175258557
50 137.71219449676573 0.024452432568868 0.005264313091834386
51 137.55647897906601 0.024138630644480387 0.0051330676277478535
52 137.60918973200023 0.02496796620686849 0.005320026104648908
53 137.6054246518761 0.023952733341852824 0.0051390969455242155
54 137.6940636113286 0.023037913898626965 0.00533050041894118
55 137.61276663281024 0.023112119563420613 0.004847671747207642
56 137.53161862120032 0.023473044204711913 0.005219404632846514
57 136.7024707980454 0.02192030504544576 0.00461024193962415
58 136.76605016365647 0.023088040069739024 0.004902095854282379
59 136.6899768449366 0.02312225875854492 0.004792712489763895
60 136.76296169124544 0.023029890139897664 0.00473209269841512
61 136.8637042120099 0.02179530868927638 0.0045787608524163565
62 136.7974533252418 0.021934033914407094 0.004444991563757261
63 136.74910452775657 0.020953610050678254 0.004672189364830653
64 136.82580623030663 0.02131037126382192 0.0049163332482179
65 136.91114787757397 0.021373182968298595 0.004795874754587809
66 137.32485774159431 0.021481007540225982 0.0048194362272818885
67 137.65347783081234 0.0218330881913503 0.005281802147626877
68 137.66409330815077 0.02100781639019648 0.004352680658300718
69 137.50898801349103 0.020462376721700034 0.0047049711843331656
70 137.7015827614814 0.02090779960950216 0.0043787485261758165
71 137.56689404323697 0.020341756772994993 0.004530048698186875
72 137.65152325294912 0.0203349817554156 0.004687931314110756
73 137.546372320503 0.020132521982987723 0.004401695663730303
74 137.70611072145402 0.019871939977010093 0.004318642745415369
75 137.41071655601263 0.020513547094662986 0.004329795112212499
76 137.02885054424405 0.019863178062438966 0.004384591435392698
77 136.76022246293724 0.01938095130523046 0.004720946485797564
78 136.65963688492775 0.019652529442310333 0.004631875604391098
79 136.83120126836002 0.019832804544766745 0.004311790590484937
80 136.75155792944133 0.01899814738035202 0.004602161760131518
81 136.6953156068921 0.019315600128968557 0.004720707729458809
82 136.96981511637568 0.020459790925184886 0.0042937570263942085
83 137.214050732553 0.01897865850130717 0.004704298625389735
84 137.3455085027963 0.018519263875484467 0.004262243469556172
85 137.70491838641465 0.018909358310699464 0.004113335331281026
86 137.66764270327985 0.01823272135655085 0.004383731866876284
87 137.69643758423626 0.019376803199450177 0.0040728359868129095
88 137.7513445354998 0.018983380230267844 0.004005580196777979
89 137.64447511173785 0.018196211206912994 0.004287866339087486
90 137.7420925348997 0.018639132634798685 0.004200956289966901
91 137.58862195163965 0.01781221868991852 0.004008458659052849
92 137.68837262876332 0.017487182784080506 0.004033586780230205
93 137.64672742970288 0.017851483841737112 0.004315155973037084
94 137.6020113248378 0.017276487493515014 0.004077878713607788
95 137.66101701557636 0.017433558134237925 0.004194578111171722
96 136.7531397715211 0.018459876569112143 0.004253643477956454
97 136.74071680754423 0.017620970610777536 0.004486094236373902
98 136.7141683883965 0.017372157192230224 0.004010359873374303
99 136.85186271741986 0.017491739042599996/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
 0.003944888556996981
100 136.8508027587086 0.017400747406482696 0.004049464498956998
101 136.73760380595922 0.017552924915154776 0.0039608840694030124
102 136.891746500507 0.017248065265019735 0.004079101105531057
103 137.18277298286557 0.017302294663588205 0.003909550294280052
104 137.64470931887627 0.01673459114631017 0.004255525956551234
105 137.5508612524718 0.016979725511868795 0.004056568682193756
106 137.6285329964012 0.016613487382729848 0.004100836823383967
107 137.6360417213291 0.01718594093720118 0.0038315418114264806
108 137.72092759795487 0.016370693600177764 0.0038337745020786923
109 137.55530399642885 0.016724754798412324 0.004007549678285917
110 137.7022672407329 0.016757075107097626 0.0040474496533473335
111 137.54117760062218 0.016476843909422556 0.003835360174377759
112 137.62857623584569 0.016509235004583993 0.003810458903511365
113 137.95062258280814 0.016127297170956928 0.003972620079914729
114 137.7898603770882 0.016678935368855796 0.003848143090804418
115 136.8060629107058 0.01689651071627935 0.00410623075067997
116 136.92867701128125 0.01602019253174464 0.0036624539842208224
117 136.77802260406315 0.015958094096183775 0.0040398206661144895
118 136.74987354129553 0.016294954200585684 0.0038025209804375968
119 136.7094334270805 0.016374687520662942 0.004027413402994473
120 136.7659313492477 0.01651595011552175 0.003886211042602857
121 136.82237728126347 0.01598319278160731 0.0038821254422267276
122 136.76600087992847 0.015560283370812734 0.003725720266501109
123 136.92349391244352 0.01539220803976059 0.00389558116098245
124 137.34708069823682 0.015499466081460318 0.0038811364422241847
125 137.64974162727594 0.016355121235052745 0.004309428721666336
126 137.6544337682426 0.01630776136716207 0.003636069655418396
127 137.63485249131918 0.015580341561635335 0.003735738068819046
128 137.6982718538493 0.016130902568499247 0.004084379265705745
129 137.61973834224045 0.01512760068178177 0.00378427092730999
130 137.68682133220136 0.015286946086088816 0.0037124149352312087
131 137.59477995336056 0.01585354917049408 0.003656270201007525
132 137.69173040241003 0.015500862773259481 0.0038539431244134902
133 137.55655114725232 0.015666238669554394 0.0037077857156594595
134 137.54154936410487 0.01508434886932373 0.003915723373492559
135 137.6601688694209 0.01563447519938151 0.003788939322034518
136 137.61748968809843 0.014701038360595704 0.003630470509330432
137 136.92794668115675 0.015256368140379587 0.003898938755194346
138 136.642840154469 0.014878152163823445 0.0036769210547208785
139 136.81485753692687 0.01511478258371353 0.003504464174310366
140 136.75680245272815 0.014692207753658294 0.0038656811465819676
141 136.76794510148466 0.014762043885389963 0.003650982548793157
142 136.68643268942833 0.014953291376431784 0.003570627252260844
143 136.72986848652363 0.015369685626029968 0.003979046881198883
144 136.77109502255917 0.014754257329305014 0.0036384267657995225
145 136.96602152846754 0.014726824021339416 0.0035383988320827485
146 137.56132574006915 0.014400238394737244 0.0035289237350225448
147 137.50344511121511 0.015309828730424245 0.0035168026983737944
148 137.6175685916096 0.014326807928085328 0.003734889010588328
149 137.5495751053095 0.01464348666270574 0.0035576555381218594
Early stop at epoch 150
Training Time:20732.036s
