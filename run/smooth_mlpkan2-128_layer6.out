nohup: ignoring input
begin to read data
Data Loading with 9.451 s
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
deeponet                                 --
├─FNN: 1-1                               --
│    └─SiLU: 2-1                         --
│    └─KAN: 2-2                          --
│    │    └─GELU: 3-1                    --
│    │    └─ParameterList: 3-2           8,192
│    │    └─ParameterList: 3-3           65,536
│    │    └─ModuleList: 3-4              8,192
│    │    └─ModuleList: 3-5              1
├─FNO2d: 1-2                             --
│    └─GELU: 2-3                         --
│    └─Linear: 2-4                       64
│    └─ModuleList: 2-5                   --
│    │    └─SpectralConv2d: 3-6          663,552
│    │    └─SpectralConv2d: 3-7          663,552
│    │    └─SpectralConv2d: 3-8          663,552
│    │    └─SpectralConv2d: 3-9          663,552
│    │    └─SpectralConv2d: 3-10         663,552
│    │    └─SpectralConv2d: 3-11         663,552
│    └─ModuleList: 2-6                   --
│    │    └─Conv2d: 3-12                 1,056
│    │    └─Conv2d: 3-13                 1,056
│    │    └─Conv2d: 3-14                 1,056
│    │    └─Conv2d: 3-15                 1,056
│    │    └─Conv2d: 3-16                 1,056
│    │    └─Conv2d: 3-17                 1,056
│    └─Linear: 2-7                       4,224
│    └─Linear: 2-8                       516
=================================================================
Total params: 4,074,373
Trainable params: 4,074,373
Non-trainable params: 0
=================================================================
Training...
0 102.00378435663879 1.3547061069488526 0.06158877507845561
1 103.45135453343391 0.42688554331461587 0.043708696842193605
2 103.43735472112894 0.2963932029724121 0.04214605991045634
3 103.7570772152394 0.2455994457244873 0.03494393138090769
4 103.04354453459382 0.2075641170501709 0.038455079038937884
5 102.9652493800968 0.18360631551742554 0.028578481356302896
6 103.03051000274718 0.16550934130350747 0.02695902117093404
7 103.71562477014959 0.15049388596216837 0.025038142959276834
8 103.59684092178941 0.14493480106989542 0.023443821708361307
9 103.61421528272331 0.13210774374008177 0.023953612248102826
10 103.55066270381212 0.1272664311091105 0.022042617241541544
11 103.4993139039725 0.12371211296717326 0.021372775693734487
12 103.71103743836284 0.11677754255930582 0.020908944010734558
13 103.68673733621836 0.11525552374521891 0.020492453277111054
14 103.58711251243949 0.11059002262751261 0.019450444181760153
15 103.48570071719587 0.10878674294153849 0.019740028540293376
16 103.49887809716165 0.10519970483779907 0.01892751475175222
17 103.78454777039587 0.10280682957967123 0.018518829226493835
18 103.55073968321085 0.09965413872400919 0.01812411336104075
19 103.56152552738786 0.09792317069371541 0.01785257436831792
20 103.45918977633119 0.09678895174662272 0.017533333162466684
21 102.88476287573576 0.09491651961008708 0.016915787478288015
22 102.58123828470707 0.09180937643051147 0.01692318042119344
23 102.55318196862936 0.09010609793663026 0.015999307890733082
24 102.55922829173505 0.08790485728581747 0.015664725383122764
25 102.58220276609063 0.0871254200776418 0.016092046519120533
26 102.67183062061667 0.08486346367200216 0.015303897082805634
27 102.71953259594738 0.08378523899714152 0.015078657706578572
28 102.6885943710804 0.0823547644297282 0.014549982289473216
29 102.67364491149783 0.08202526167233785 0.014684255222479502
30 102.76358057744801 0.0807146523475647 0.014500553985436758
31 103.08185557834804 0.07882122346560161 0.01455694462855657
32 103.69710884802043 0.07811316350301106 0.014631699045499167
33 103.58132739178836 0.07671121837298076 0.01433264317115148
34 103.51987704075873 0.07585635557174683 0.013859598914782206
35 103.71872957982123 0.07610099109013875 0.01347909919420878
36 104.67111144773662 0.0739803058942159 0.013615522046883901
37 104.02425455860794 0.07357269752820333 0.013615516801675161
38 103.60370343923569 0.07255198882420857 0.013559079468250275
39 103.47648436389863 0.07005299485524495 0.013523788869380951
40 103.57891938276589 0.07179400013287862 0.013395806908607482
41 103.93152485229075 0.06918380781809488 0.013124237279097239
42 103.66513208299875 0.06789237702687581 0.012662920912106832
43 103.44958020187914 0.06873067626953125 0.01257577532529831
44 103.47013656795025 0.06682521454493205 0.013275407632191975
45 103.45035398565233 0.06777645545005799 0.01254537816842397
46 103.12686469219625 0.0642672029654185 0.012369605402151743
47 102.5314407683909 0.06461691757837931 0.012126906494299572
48 102.39279096201062 0.06443318087259928 0.01269516388575236
49 102.37283997051418 0.06420468905766805 0.01231274139881134
50 102.4956014174968 0.06342071034113567 0.012058834532896677
51 102.38117673806846 0.06298637337684632 0.011814959486325582
52 102.46620830520988 0.06288757095336914 0.01221241444349289
53 102.60203135572374 0.06091233673095703 0.011698232452074686
54 102.71141871064901 0.06103868363698323 0.01196545253197352
55 102.76241567544639 0.05987750264803569 0.011688307662804922
56 102.93526116199791 0.06039038624763489 0.01139491065343221
57 103.16613471135497 0.06029781587918599 0.011663191080093383
58 103.5301893800497 0.058613107808430986 0.011480422755082448
59 103.55276747606695 0.059908764982223514 0.01112708016236623
60 103.54053326882422 0.05779363471666972 0.011109008838733038
61 103.97189742140472 0.0572385434627533 0.011390355467796326
62 103.67813070863485 0.05794478023846944 0.010921584258476892
63 103.47245284356177 0.056849706808725996 0.010903799136479696
64 103.54883453808725 0.05611467429796855 0.011139630148808161
65 103.45775588788092 0.056400720342000325 0.011214905500411987
66 103.47219009511173 0.05611207118034363 0.010931079000234604
67 103.49156688340008 0.05468673178354899 0.011226451456546783
68 103.54692552424967 0.0560457057317098 0.01108903823296229
69 103.4463622868061 0.0559271872361501 0.01116765809059143
70 103.53401268459857 0.054397516695658364 0.01062069637576739
71 103.13665648922324 0.054180601835250856 0.010937908718983333
72 102.65130799263716 0.05223992568651835 0.010565502415100734
73 102.79755327105522 0.054671928024291994 0.010665890355904897
74 102.72940267808735 0.05390064536730448 0.010524885237216949
75 102.84099898301065 0.05336521838506063 0.010485849370559056
76 102.50988560169935 0.051620076020558674 0.01054550458987554
77 102.40321992710233 0.05378399715423584 0.010585692812999089
78 102.51226039603353 0.05198567228317261 0.010363018751144409
79 102.50820262730122 0.051126376374562583 0.010668639034032822
80 102.80845969542861 0.05085459059079488 0.010067337512969971
81 102.85467247106135 0.05165955432256063 0.010107814331849416
82 102.94114044867456 0.051230097659428916 0.010700617730617524
83 103.29205746203661 0.05105440160433451 0.009984438439210257
84 103.63038709573448 0.05114831838607788 0.010290477782487869
85 103.85957227274776 0.05075859045982361 0.010269852101802826
86 103.60517386160791 0.05049458492596944 0.009916238139073054
87 103.46268602833152 0.05015983538627625 0.010431158552567163
88 103.52038024924695 0.049533759148915606 0.009926803211371104
89 103.38107942417264 0.04875853883425395 0.009938679416974386
90 103.47387094050646 0.048640942351023356 0.010138411571582158
91 103.40204526297748 0.049453331963221235 0.009916884491840998
92 103.5611201543361 0.04954674790700277 0.009865486274162928
93 103.48697100579739 0.04862594995498657 0.009694987048705419
94 103.95649305358529 0.0479035663763682 0.009610738317171732
95 103.68718062154949 0.04871837652524312 0.009889552593231201
96 103.4597415626049 0.04862621088027954 0.009823734005292257
97 103.56583028659225 0.048235903278986615 0.010026962429285049
98 103.48685176298022 0.04716433626810709 0.009720856149991353
99 103.39364010095596 0.04740588375727336 0.009596825768550237
100 103.092682948336/home/fengw666/.conda/envs/torch_radon/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
 0.04785672203699748 0.009760329912106196
101 102.45371255464852 0.04574194753964742 0.00972876986861229
102 102.41224041208625 0.04660329456329346 0.009629377126693726
103 102.50887897051871 0.04762147777875265 0.009647833347320557
104 102.67074021697044 0.046869301160176596 0.009286879966656367
105 102.52512551099062 0.045232334597905474 0.00970569223165512
106 102.40055055171251 0.046394042253494264 0.010400159984827042
107 102.4071045126766 0.04659766224225362 0.00962750373284022
108 102.4732800591737 0.04523453288873037 0.009498869886000951
109 102.72416954673827 0.04551501868565877 0.009408368239800135
110 102.67532975785434 0.045386416935920715 0.009282653311888376
111 102.57431896217167 0.045151220115025835 0.009754329472780227
112 102.79887037910521 0.045551571973164874 0.009478523453076681
113 102.99647171236575 0.044912847725550334 0.00930832788348198
114 103.32646614871919 0.04445400644143423 0.009250662952661514
115 103.53797100111842 0.04437565756638845 0.009240736802419026
116 103.6183784045279 0.04418641208012899 0.009446996718645096
117 103.57009159587324 0.04500424859523773 0.0090398628115654
118 103.64114675670862 0.04337031009197235 0.009258423556884129
119 104.0278410576284 0.044505414668718976 0.009099059730768204
120 103.46269463002682 0.04291045035521189 0.009692596465349198
121 103.40844758972526 0.043809064205487566 0.009305143594741822
122 103.4403233397752 0.04260953637758891 0.009642504851023356
123 103.86929542012513 0.044221492067972816 0.009638941049575805
124 103.60993569716811 0.04309924578666687 0.009162337531646093
125 103.57854801416397 0.04292413533528646 0.009007383505503336
126 103.45295370183885 0.04193454009691874 0.009119850397109986
127 103.46536220610142 0.04242238454818725 0.009359865377346675
128 103.79989049397409 0.0437410404920578 0.00890493165453275
129 103.17013968341053 0.04169862628777822 0.009124970644712448
130 102.83291455358267 0.0423959346930186 0.008865495552619299
131 102.40445833280683 0.04217102415561676 0.009198174585898717
132 102.42475010640919 0.042367927765846254 0.00917356812953949
133 103.4122475348413 0.041943837348620096 0.008765461812416712
134 103.46631874516606 0.04191564670403798 0.00942920775214831
135 102.83278534933925 0.04247897066275279 0.008968637257814407
136 102.69541749358177 0.041284692390759783 0.008763959417740504
137 103.10972149297595 0.04133057423432668 0.008847392012675603
138 103.31865273229778 0.041427797849973046 0.008975790520509085
139 102.97611153684556 0.04165404829978943 0.009109906444946924
140 103.32453489303589 0.0409490053097407 0.00906140481432279
141 103.67544956319034 0.04095137516657511 0.008849431306123734
142 104.02880118228495 0.04083939510981242 0.009066394299268723
143 104.07703610509634 0.04134016031424204 0.008829507370789845
144 103.67705194093287 0.04035707393487294 0.008628983318805695
145 103.52768522128463 0.040032647736867266 0.008895206948121388
146 103.52148264087737 0.041036524788538614 0.009121349682410558
147 103.8497129753232 0.040184923656781515 0.008902123222748439
148 103.65237444080412 0.04068356188933055 0.009270317306121191
149 103.5457110684365 0.04037953843275706 0.00883078317840894
150 103.67325499467552 0.04052976623376211 0.008837957441806793
151 103.57476393505931 0.03903845896720886 0.008602474133173625
152 104.20134972594678 0.04046416583855947 0.008716996024052301
153 103.76306427270174 0.03875627383391062 0.008781671226024627
154 103.57104481942952 0.039192230693499246 0.008684527456760406
155 103.42552727088332 0.039759236574172976 0.008807344406843186
156 103.14030764810741 0.039470852295557655 0.00862099735935529
157 102.94081993959844 0.03883881577650706 0.008513246099154154
158 102.57064429298043 0.039485787026087446 0.008770778467257817
159 102.45909603498876 0.039041956559816994 0.00866608833273252
160 102.54561255127192 0.039316777046521506 0.008711218396822611
161 102.41491173580289 0.039163518476486206 0.008773468484481175
162 102.38757657259703 0.03842429912090301 0.008861100087563197
163 102.46237489581108 0.03904924555619558 0.00883247078458468
164 102.48305069282651 0.03897525366942088 0.008757717688878378
165 102.42910271324217 0.03792992164293925 0.008826586713393529
166 102.46047244034708 0.03791596557299296 0.008666559437910715
167 103.15831786952913 0.03951931936740875 0.009126360714435577
168 102.7122095786035 0.038336921334266666 0.008450118045012157
169 103.06878339126706 0.03797664043903351 0.008557743012905121
170 103.42026232928038 0.03843148508866628 0.008803211609522502
171 103.50897374004126 0.03752735600471496 0.008458697845538458
172 103.91581109724939 0.03770826139450073 0.009560288548469544
173 103.56198735907674 0.03906858750184377 0.008647186448176702
174 103.65734745375812 0.03747529099782308 0.008436859875917435
175 103.48644901625812 0.038063124140103655 0.008506725827852885
176 103.91379680670798 0.03805826683044434 0.008600021799405416
177 103.70691834762692 0.03700750490029653 0.008543426841497422
178 103.47066145949066 0.03725863149166107 0.00872754003604253
179 103.54264553263783 0.03811819127400716 0.008270242263873418
180 103.5619378760457 0.03716220082441966 0.008503928403059642
181 103.96251832135022 0.03750334038734436 0.00872282796104749
182 103.64115713350475 0.037485697420438134 0.008583598295847575
183 103.51206982880831 0.03736193021138509 0.008635841151078542
184 103.55797521956265 0.03738268709182739 0.009148988395929336
185 103.47381364740431 0.036595431900024414 0.008384100447098414
186 103.2316532228142 0.03652162040869395 0.008408846795558929
187 102.48716973885894 0.03615048370361328 0.008362588504950206
188 102.52632778324187 0.03649104662736257 0.008419198274612428
189 102.46749148704112 0.03609651321570079 0.008381262550751369
Early stop at epoch 190
Training Time:19726.133s
